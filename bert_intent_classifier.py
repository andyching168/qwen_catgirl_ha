#!/usr/bin/env python3
"""
BERT æ„åœ–åˆ†é¡å™¨æ¨ç†æ¨¡çµ„

å¯ç¨ç«‹ä½¿ç”¨æˆ–æ•´åˆåˆ° qwen_model_server.py
"""

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from typing import Tuple, Optional
import time

# æ¨™ç±¤å®šç¾©
LABEL_NAMES = ["turn_on", "turn_off", "climate_set_mode", "get_state", "chat"]

class BertIntentClassifier:
    """BERT æ„åœ–åˆ†é¡å™¨"""
    
    def __init__(
        self, 
        model_path: str = "bert_intent_model",
        device: Optional[str] = None,
        confidence_threshold: float = 0.85
    ):
        """
        åˆå§‹åŒ–åˆ†é¡å™¨
        
        Args:
            model_path: è¨“ç·´å¥½çš„æ¨¡å‹è·¯å¾‘
            device: é‹è¡Œè¨­å‚™ (cuda/mps/cpu)ï¼ŒNone ç‚ºè‡ªå‹•æª¢æ¸¬
            confidence_threshold: é«˜ç¢ºå®šæ€§é–¾å€¼
        """
        self.confidence_threshold = confidence_threshold
        
        # è‡ªå‹•æª¢æ¸¬è¨­å‚™
        if device is None:
            if torch.cuda.is_available():
                self.device = "cuda"
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                self.device = "mps"
            else:
                self.device = "cpu"
        else:
            self.device = device
        
        print(f"ğŸ”§ BERT æ„åœ–åˆ†é¡å™¨åˆå§‹åŒ–")
        print(f"   æ¨¡å‹è·¯å¾‘: {model_path}")
        print(f"   é‹è¡Œè¨­å‚™: {self.device}")
        
        # è¼‰å…¥æ¨¡å‹
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
        self.model.to(self.device)
        self.model.eval()
        
        print(f"   è¼‰å…¥å®Œæˆ âœ…")
    
    def predict(self, text: str) -> Tuple[str, float]:
        """
        é æ¸¬æ„åœ–
        
        Args:
            text: ç”¨æˆ¶è¼¸å…¥æ–‡å­—
            
        Returns:
            (intent, confidence): æ„åœ–åç¨±å’Œä¿¡å¿ƒåˆ†æ•¸
        """
        # Tokenize
        inputs = self.tokenizer(
            text,
            max_length=64,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        ).to(self.device)
        
        # æ¨ç†
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=-1)
            
            confidence, predicted_id = torch.max(probs, dim=-1)
            intent = LABEL_NAMES[predicted_id.item()]
            
        return intent, confidence.item()
    
    def should_use_llm(self, text: str) -> Tuple[bool, str, float]:
        """
        åˆ¤æ–·æ˜¯å¦éœ€è¦ä½¿ç”¨ LLM
        
        Args:
            text: ç”¨æˆ¶è¼¸å…¥æ–‡å­—
            
        Returns:
            (should_use_llm, intent, confidence)
        """
        intent, confidence = self.predict(text)
        
        # é«˜ç¢ºå®šæ€§ä¸”éèŠå¤© â†’ ä¸éœ€è¦ LLM
        if confidence >= self.confidence_threshold and intent != "chat":
            return False, intent, confidence
        
        # éœ€è¦ LLM è™•ç†
        return True, intent, confidence
    
    def benchmark(self, texts: list, num_runs: int = 100):
        """æ•ˆèƒ½æ¸¬è©¦"""
        print(f"\nâ±ï¸  æ•ˆèƒ½æ¸¬è©¦ ({num_runs} æ¬¡)")
        
        # é ç†±
        for _ in range(10):
            self.predict(texts[0])
        
        # è¨ˆæ™‚
        start = time.time()
        for _ in range(num_runs):
            for text in texts:
                self.predict(text)
        
        elapsed = time.time() - start
        total_predictions = num_runs * len(texts)
        avg_ms = (elapsed / total_predictions) * 1000
        
        print(f"   ç¸½é æ¸¬æ•¸: {total_predictions}")
        print(f"   ç¸½è€—æ™‚: {elapsed:.2f}s")
        print(f"   å¹³å‡å»¶é²: {avg_ms:.2f}ms")
        
        return avg_ms


def main():
    """æ¸¬è©¦æ¨ç†"""
    import argparse
    
    parser = argparse.ArgumentParser(description="BERT æ„åœ–åˆ†é¡å™¨æ¸¬è©¦")
    parser.add_argument("--model", type=str, default="bert_intent_model", help="æ¨¡å‹è·¯å¾‘")
    parser.add_argument("--benchmark", action="store_true", help="åŸ·è¡Œæ•ˆèƒ½æ¸¬è©¦")
    args = parser.parse_args()
    
    # åˆå§‹åŒ–åˆ†é¡å™¨
    classifier = BertIntentClassifier(model_path=args.model)
    
    # æ¸¬è©¦æ¨£æœ¬
    test_texts = [
        "æ‰“é–‹æ›¸æˆ¿ç‡ˆ",
        "é—œæ‰å®¢å»³ç‡ˆ",
        "å†·æ°£é–‹è‘—å—",
        "æŠŠå†·æ°£è¨­å®šæˆå†·æ°£æ¨¡å¼",
        "ä½ å¥½",
        "ä»Šå¤©å¤©æ°£å¦‚ä½•",
        "é–‹é¢¨æ‰‡",
        "é—œå†·æ°£",
        "ç‡ˆäº®è‘—å—",
        "è¨­å®šæš–æ°£æ¨¡å¼",
    ]
    
    print("\nğŸ“‹ æ¸¬è©¦é æ¸¬:")
    for text in test_texts:
        intent, confidence = classifier.predict(text)
        use_llm, _, _ = classifier.should_use_llm(text)
        llm_tag = "â†’ LLM" if use_llm else "â†’ ç›´æ¥è™•ç†"
        print(f"   [{intent:>16}] ({confidence:.2%}) {text} {llm_tag}")
    
    if args.benchmark:
        classifier.benchmark(test_texts)
    
    print("\nâœ… æ¸¬è©¦å®Œæˆï¼")


if __name__ == "__main__":
    main()
