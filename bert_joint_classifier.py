#!/usr/bin/env python3
"""
BERT æ„åœ–åˆ†é¡ + å¡«æ§½ æ¨ç†æ¨¡çµ„

å¯ç¨ç«‹ä½¿ç”¨æˆ–æ•´åˆåˆ° qwen_model_server.py
"""

import json
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel, AutoConfig
from typing import Dict, List, Tuple, Optional
import time

class JointIntentSlotModel(nn.Module):
    """è¯åˆæ„åœ–åˆ†é¡ + å¡«æ§½æ¨¡å‹ï¼ˆèˆ‡è¨“ç·´è…³æœ¬ç›¸åŒçµæ§‹ï¼‰"""
    
    def __init__(self, model_name: str, num_intents: int, slot_types: List[str]):
        super().__init__()
        
        self.slot_types = slot_types
        self.num_slots = len(slot_types)
        
        self.config = AutoConfig.from_pretrained(model_name)
        self.bert = AutoModel.from_pretrained(model_name)
        hidden_size = self.config.hidden_size
        
        self.intent_classifier = nn.Sequential(
            nn.Dropout(0.1),
            nn.Linear(hidden_size, num_intents)
        )
        
        self.slot_start = nn.ModuleDict({
            slot: nn.Linear(hidden_size, 1) for slot in slot_types
        })
        self.slot_end = nn.ModuleDict({
            slot: nn.Linear(hidden_size, 1) for slot in slot_types
        })
    
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        sequence_output = outputs.last_hidden_state
        pooled_output = sequence_output[:, 0, :]
        
        intent_logits = self.intent_classifier(pooled_output)
        
        slot_start_logits = {}
        slot_end_logits = {}
        
        for slot in self.slot_types:
            start_logits = self.slot_start[slot](sequence_output).squeeze(-1)
            end_logits = self.slot_end[slot](sequence_output).squeeze(-1)
            
            start_logits = start_logits + (1 - attention_mask.float()) * -10000.0
            end_logits = end_logits + (1 - attention_mask.float()) * -10000.0
            
            slot_start_logits[slot] = start_logits
            slot_end_logits[slot] = end_logits
        
        return {
            "intent_logits": intent_logits,
            "slot_start_logits": slot_start_logits,
            "slot_end_logits": slot_end_logits,
        }


class BertJointClassifier:
    """BERT æ„åœ–åˆ†é¡ + å¡«æ§½ æ¨ç†å™¨"""
    
    def __init__(
        self, 
        model_path: str = "bert_joint_model",
        device: Optional[str] = None,
        confidence_threshold: float = 0.85
    ):
        self.confidence_threshold = confidence_threshold
        
        # è‡ªå‹•æª¢æ¸¬è¨­å‚™
        if device is None:
            if torch.cuda.is_available():
                self.device = "cuda"
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                self.device = "mps"
            else:
                self.device = "cpu"
        else:
            self.device = device
        
        print(f"ğŸ”§ BERT è¯åˆåˆ†é¡å™¨åˆå§‹åŒ–")
        print(f"   æ¨¡å‹è·¯å¾‘: {model_path}")
        print(f"   é‹è¡Œè¨­å‚™: {self.device}")
        
        # è¼‰å…¥é…ç½®
        with open(f"{model_path}/config.json", 'r') as f:
            config = json.load(f)
        
        self.intent_names = config["intent_names"]
        self.slot_types = config["slot_types"]
        self.num_intents = config["num_intents"]
        base_model = config["model_name"]
        
        print(f"   æ„åœ–é¡åˆ¥: {self.intent_names}")
        print(f"   Slot é¡å‹: {self.slot_types}")
        
        # è¼‰å…¥ tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        # è¼‰å…¥æ¨¡å‹
        self.model = JointIntentSlotModel(
            model_name=base_model,
            num_intents=self.num_intents,
            slot_types=self.slot_types,
        )
        self.model.load_state_dict(torch.load(f"{model_path}/model.pt", map_location=self.device))
        self.model.to(self.device)
        self.model.eval()
        
        print(f"   è¼‰å…¥å®Œæˆ âœ…")
    
    def predict(self, text: str) -> Dict:
        """
        é æ¸¬æ„åœ–å’Œå¡«æ§½
        
        Args:
            text: ç”¨æˆ¶è¼¸å…¥æ–‡å­—
            
        Returns:
            {
                "intent": "turn_on",
                "intent_confidence": 0.95,
                "slots": {"name": "å¤§ç‡ˆ", "area": "æ›¸æˆ¿"},
                "raw_text": "æ‰“é–‹æ›¸æˆ¿å¤§ç‡ˆ"
            }
        """
        # Tokenize
        encoding = self.tokenizer(
            text,
            max_length=64,
            padding="max_length",
            truncation=True,
            return_offsets_mapping=True,
            return_tensors="pt"
        )
        
        input_ids = encoding["input_ids"].to(self.device)
        attention_mask = encoding["attention_mask"].to(self.device)
        offset_mapping = encoding["offset_mapping"][0].tolist()
        
        # æ¨ç†
        with torch.no_grad():
            outputs = self.model(input_ids, attention_mask)
            
            # Intent
            intent_probs = torch.softmax(outputs["intent_logits"], dim=-1)
            intent_confidence, intent_id = torch.max(intent_probs, dim=-1)
            intent = self.intent_names[intent_id.item()]
            
            # Slots
            slots = {}
            for slot in self.slot_types:
                start_logits = outputs["slot_start_logits"][slot][0]
                end_logits = outputs["slot_end_logits"][slot][0]
                
                start_idx = torch.argmax(start_logits).item()
                end_idx = torch.argmax(end_logits).item()
                
                # å¦‚æœ start å’Œ end éƒ½æ˜¯ 0ï¼ˆCLSï¼‰ï¼Œè¡¨ç¤ºè©² slot ä¸å­˜åœ¨
                if start_idx == 0 and end_idx == 0:
                    continue
                
                # ç¢ºä¿ end >= start
                if end_idx < start_idx:
                    end_idx = start_idx
                
                # å¾ offset_mapping æå–åŸæ–‡
                if start_idx < len(offset_mapping) and end_idx < len(offset_mapping):
                    char_start = offset_mapping[start_idx][0]
                    char_end = offset_mapping[end_idx][1]
                    
                    if char_start < char_end:
                        slot_value = text[char_start:char_end]
                        slots[slot] = slot_value
        
        return {
            "intent": intent,
            "intent_confidence": intent_confidence.item(),
            "slots": slots,
            "raw_text": text,
        }
    
    def build_action(self, result: Dict) -> Optional[Dict]:
        """
        æ ¹æ“šé æ¸¬çµæœå»ºæ§‹ ACTION
        
        Returns:
            None å¦‚æœæ˜¯ chat æˆ–ä¿¡å¿ƒä¸è¶³
            å¦å‰‡è¿”å› ACTION çµæ§‹
        """
        if result["intent"] == "chat":
            return None
        
        if result["intent_confidence"] < self.confidence_threshold:
            return None
        
        action = {
            "action": result["intent"],
            "params": result["slots"],
            "confidence": result["intent_confidence"],
        }
        
        return action
    
    def should_use_llm(self, text: str) -> Tuple[bool, Dict]:
        """
        åˆ¤æ–·æ˜¯å¦éœ€è¦ä½¿ç”¨ LLM
        
        Returns:
            (should_use_llm, prediction_result)
        """
        result = self.predict(text)
        action = self.build_action(result)
        
        if action is None:
            return True, result
        
        # æª¢æŸ¥å¿…è¦åƒæ•¸
        intent = result["intent"]
        slots = result["slots"]
        
        # æ ¹æ“šæ„åœ–æª¢æŸ¥å¿…è¦çš„ slots
        required_slots = {
            "turn_on": ["name"],
            "turn_off": ["name"],
            "get_state": ["name"],
            "climate_set_mode": ["mode"],
        }
        
        if intent in required_slots:
            for required_slot in required_slots[intent]:
                if required_slot not in slots or not slots[required_slot]:
                    # ç¼ºå°‘å¿…è¦åƒæ•¸ï¼Œäº¤çµ¦ LLM
                    return True, result
        
        return False, result
    
    def benchmark(self, texts: list, num_runs: int = 100):
        """æ•ˆèƒ½æ¸¬è©¦"""
        print(f"\nâ±ï¸  æ•ˆèƒ½æ¸¬è©¦ ({num_runs} æ¬¡)")
        
        # é ç†±
        for _ in range(10):
            self.predict(texts[0])
        
        start = time.time()
        for _ in range(num_runs):
            for text in texts:
                self.predict(text)
        
        elapsed = time.time() - start
        total_predictions = num_runs * len(texts)
        avg_ms = (elapsed / total_predictions) * 1000
        
        print(f"   ç¸½é æ¸¬æ•¸: {total_predictions}")
        print(f"   ç¸½è€—æ™‚: {elapsed:.2f}s")
        print(f"   å¹³å‡å»¶é²: {avg_ms:.2f}ms")
        
        return avg_ms


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="BERT è¯åˆåˆ†é¡å™¨æ¸¬è©¦")
    parser.add_argument("--model", type=str, default="bert_joint_model")
    parser.add_argument("--benchmark", action="store_true")
    parser.add_argument("-i", "--interactive", action="store_true", help="äº’å‹•æ¨¡å¼")
    args = parser.parse_args()
    
    classifier = BertJointClassifier(model_path=args.model)
    
    # äº’å‹•æ¨¡å¼
    if args.interactive:
        print("\nğŸ¤ äº’å‹•æ¨¡å¼ï¼ˆè¼¸å…¥ 'q' æˆ– 'exit' é€€å‡ºï¼‰")
        print("-" * 50)
        
        while True:
            try:
                text = input("\nè«‹è¼¸å…¥: ").strip()
                if not text:
                    continue
                if text.lower() in ['q', 'exit', 'quit']:
                    print("ğŸ‘‹ å†è¦‹ï¼")
                    break
                
                result = classifier.predict(text)
                use_llm, _ = classifier.should_use_llm(text)
                
                llm_tag = "â†’ LLM" if use_llm else "âœ“ ç›´æ¥è™•ç†"
                slots_str = json.dumps(result["slots"], ensure_ascii=False)
                
                print(f"   æ„åœ–: {result['intent']} ({result['intent_confidence']:.2%})")
                print(f"   Slots: {slots_str}")
                print(f"   {llm_tag}")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å†è¦‹ï¼")
                break
            except Exception as e:
                print(f"âŒ éŒ¯èª¤: {e}")
        
        return
    
    # é è¨­æ¸¬è©¦æ¨¡å¼
    test_texts = [
        "æ‰“é–‹æ›¸æˆ¿å¤§ç‡ˆ",
        "é—œæ‰å®¢å»³ç‡ˆ",
        "å†·æ°£é–‹è‘—å—",
        "æŠŠå†·æ°£è¨­å®šæˆå†·æ°£æ¨¡å¼",
        "ä½ å¥½",
        "é–‹é¢¨æ‰‡",
        "é—œè‡¥å®¤å†·æ°£",
        "ç‡ˆäº®è‘—å—",
        "è¨­å®šæš–æ°£æ¨¡å¼",
        "å®¢å»³ç‡ˆé—œä¸€ä¸‹",
    ]
    
    print("\nğŸ“‹ æ¸¬è©¦é æ¸¬:")
    print("-" * 70)
    
    for text in test_texts:
        result = classifier.predict(text)
        use_llm, _ = classifier.should_use_llm(text)
        
        llm_tag = "â†’ LLM" if use_llm else "âœ“ ç›´æ¥è™•ç†"
        slots_str = json.dumps(result["slots"], ensure_ascii=False)
        
        print(f"è¼¸å…¥: {text}")
        print(f"   æ„åœ–: {result['intent']} ({result['intent_confidence']:.2%})")
        print(f"   Slots: {slots_str}")
        print(f"   {llm_tag}")
        print()
    
    if args.benchmark:
        classifier.benchmark(test_texts)
    
    print("âœ… æ¸¬è©¦å®Œæˆï¼")


if __name__ == "__main__":
    main()
