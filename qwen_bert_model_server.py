#!/usr/bin/env python3
"""
qwen_bert_model_server.py
æ•´åˆ BERT Pre-Router + Qwen LLM çš„æ¨¡å‹ä¼ºæœå™¨

æ¶æ§‹ï¼š
  ç”¨æˆ¶è«‹æ±‚ â†’ BERT æ„åœ–åˆ†é¡ + å¡«æ§½ â†’ é«˜ç¢ºå®šæ€§? â†’ ç›´æ¥è¿”å› ACTION
                                   â†“ å¦
                                 Qwen LLM è™•ç†
                                   â†“
                                å–µåŒ–å›æ‡‰
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel, AutoConfig
import uvicorn
import re
import logging
from typing import Optional, List, Dict
from datetime import datetime
from opencc import OpenCC

# ==============================================================================
# é…ç½®
# ==============================================================================
HOST = "0.0.0.0"
PORT = 8124
MAX_SEQ_LENGTH = 512

# BERT é…ç½®
BERT_MODEL_PATH = "./bert_joint_model"
BERT_CONFIDENCE_THRESHOLD = 0.90  # é«˜æ–¼æ­¤ä¿¡å¿ƒæ‰ç›´æ¥è™•ç†

# Qwen é…ç½® (ä¿æŒèˆ‡åŸç‰ˆç›¸åŒ)
QWEN_MODEL_PATH = "./qwen-catgirl-ha-switch-v2"

# ==============================================================================
# Logging
# ==============================================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==============================================================================
# å…¨å±€è®Šæ•¸
# ==============================================================================
bert_model = None
bert_tokenizer = None
bert_config = None
qwen_model = None
qwen_tokenizer = None
s2t_converter = OpenCC('s2t')

app = FastAPI(title="Qwen + BERT Catgirl Home Assistant API")

# ==============================================================================
# BERT æ¨¡å‹å®šç¾©
# ==============================================================================

class JointIntentSlotModel(nn.Module):
    """è¯åˆæ„åœ–åˆ†é¡ + å¡«æ§½æ¨¡å‹"""
    
    def __init__(self, model_name: str, num_intents: int, slot_types: List[str]):
        super().__init__()
        
        self.slot_types = slot_types
        self.config = AutoConfig.from_pretrained(model_name)
        self.bert = AutoModel.from_pretrained(model_name, use_safetensors=False)
        hidden_size = self.config.hidden_size
        
        self.intent_classifier = nn.Sequential(
            nn.Dropout(0.1),
            nn.Linear(hidden_size, num_intents)
        )
        
        self.slot_start = nn.ModuleDict({
            slot: nn.Linear(hidden_size, 1) for slot in slot_types
        })
        self.slot_end = nn.ModuleDict({
            slot: nn.Linear(hidden_size, 1) for slot in slot_types
        })
    
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        sequence_output = outputs.last_hidden_state
        pooled_output = sequence_output[:, 0, :]
        
        intent_logits = self.intent_classifier(pooled_output)
        
        slot_start_logits = {}
        slot_end_logits = {}
        
        for slot in self.slot_types:
            start_logits = self.slot_start[slot](sequence_output).squeeze(-1)
            end_logits = self.slot_end[slot](sequence_output).squeeze(-1)
            
            start_logits = start_logits + (1 - attention_mask.float()) * -10000.0
            end_logits = end_logits + (1 - attention_mask.float()) * -10000.0
            
            slot_start_logits[slot] = start_logits
            slot_end_logits[slot] = end_logits
        
        return {
            "intent_logits": intent_logits,
            "slot_start_logits": slot_start_logits,
            "slot_end_logits": slot_end_logits,
        }

# ==============================================================================
# Request/Response Models
# ==============================================================================

class Device(BaseModel):
    entityId: str
    friendlyName: str
    domain: str
    state: str
    area: Optional[str] = None
    brightnessPct: Optional[int] = None

class InferenceRequest(BaseModel):
    """è™•ç†ç”¨æˆ¶è«‹æ±‚"""
    text: str
    devices: Optional[List[Device]] = None
    language: Optional[str] = "zh"
    history: Optional[List[dict]] = None

class StateResultRequest(BaseModel):
    """v7: è™•ç† get_state äºŒæ¬¡å°è©±è«‹æ±‚"""
    original_question: str
    state_result: str
    device_name: str
    area: str

class SearchResultRequest(BaseModel):
    """æœå°‹çµæœå›æ‡‰è«‹æ±‚"""
    user_question: str
    search_result: str

class FallbackAssistantRequest(BaseModel):
    """Fallback åŠ©ç†å›æ‡‰å–µåŒ–è«‹æ±‚"""
    user_question: str
    assistant_response: str

class ActionResult(BaseModel):
    action: Optional[str] = None
    params: Dict[str, str] = {}
    response_text: str
    has_action: bool
    raw_response: Optional[str] = None
    processed_by: str = "unknown"  # "bert" æˆ– "qwen"

class SearchResultResponse(BaseModel):
    response_text: str

# ==============================================================================
# è¼‰å…¥æ¨¡å‹
# ==============================================================================

def load_bert_model():
    """è¼‰å…¥ BERT æ„åœ–åˆ†é¡ + å¡«æ§½æ¨¡å‹"""
    global bert_model, bert_tokenizer, bert_config
    
    import json
    
    logger.info("ğŸ”§ è¼‰å…¥ BERT è¯åˆåˆ†é¡å™¨...")
    
    # è¼‰å…¥é…ç½®
    with open(f"{BERT_MODEL_PATH}/config.json", 'r') as f:
        bert_config = json.load(f)
    
    logger.info(f"   æ„åœ–é¡åˆ¥: {bert_config['intent_names']}")
    logger.info(f"   Slot é¡å‹: {bert_config['slot_types']}")
    
    # è¼‰å…¥ tokenizer
    bert_tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_PATH)
    
    # è¼‰å…¥æ¨¡å‹
    device = "cuda" if torch.cuda.is_available() else "cpu"
    bert_model = JointIntentSlotModel(
        model_name=bert_config["model_name"],
        num_intents=bert_config["num_intents"],
        slot_types=bert_config["slot_types"],
    )
    bert_model.load_state_dict(torch.load(f"{BERT_MODEL_PATH}/model.pt", map_location=device))
    bert_model.to(device)
    bert_model.eval()
    
    logger.info(f"   è¨­å‚™: {device}")
    logger.info("âœ… BERT æ¨¡å‹è¼‰å…¥å®Œæˆ")
    
    return device

def load_qwen_model():
    """è¼‰å…¥ Qwen LLM æ¨¡å‹"""
    global qwen_model, qwen_tokenizer
    
    from unsloth import FastLanguageModel
    
    logger.info("ğŸš€ è¼‰å…¥ Qwen æ¨¡å‹...")
    
    qwen_model, qwen_tokenizer = FastLanguageModel.from_pretrained(
        model_name=QWEN_MODEL_PATH,
        max_seq_length=MAX_SEQ_LENGTH,
        dtype=None,
        load_in_4bit=True,
    )
    FastLanguageModel.for_inference(qwen_model)
    
    logger.info("âœ… Qwen æ¨¡å‹è¼‰å…¥å®Œæˆ")

# ==============================================================================
# BERT æ¨ç†
# ==============================================================================

def bert_predict(text: str) -> Dict:
    """ä½¿ç”¨ BERT é€²è¡Œæ„åœ–åˆ†é¡ + å¡«æ§½"""
    device = next(bert_model.parameters()).device
    
    # Tokenize
    encoding = bert_tokenizer(
        text,
        max_length=64,
        padding="max_length",
        truncation=True,
        return_offsets_mapping=True,
        return_tensors="pt"
    )
    
    input_ids = encoding["input_ids"].to(device)
    attention_mask = encoding["attention_mask"].to(device)
    offset_mapping = encoding["offset_mapping"][0].tolist()
    
    # æ¨ç†
    with torch.no_grad():
        outputs = bert_model(input_ids, attention_mask)
        
        # Intent
        intent_probs = torch.softmax(outputs["intent_logits"], dim=-1)
        intent_confidence, intent_id = torch.max(intent_probs, dim=-1)
        intent = bert_config["intent_names"][intent_id.item()]
        
        # Slots
        slots = {}
        for slot in bert_config["slot_types"]:
            start_logits = outputs["slot_start_logits"][slot][0]
            end_logits = outputs["slot_end_logits"][slot][0]
            
            start_idx = torch.argmax(start_logits).item()
            end_idx = torch.argmax(end_logits).item()
            
            if start_idx == 0 and end_idx == 0:
                continue
            
            if end_idx < start_idx:
                end_idx = start_idx
            
            if start_idx < len(offset_mapping) and end_idx < len(offset_mapping):
                char_start = offset_mapping[start_idx][0]
                char_end = offset_mapping[end_idx][1]
                
                if char_start < char_end:
                    slot_value = text[char_start:char_end]
                    slots[slot] = slot_value
    
    return {
        "intent": intent,
        "confidence": intent_confidence.item(),
        "slots": slots,
    }

# æ‡‰è©²ç›´æ¥äº¤çµ¦ LLM çš„é—œéµå­—ï¼ˆä¸é©åˆ BERT è™•ç†ï¼‰
LLM_KEYWORDS = [
    "å¤©æ°£", "ä¸‹é›¨", "æ°£æº«", "æº«åº¦å¤šå°‘",  # å¤©æ°£æŸ¥è©¢
    "å¹¾é»", "å¹¾è™Ÿ", "æ˜ŸæœŸå¹¾", "ä»Šå¤©æ—¥æœŸ",  # æ™‚é–“æŸ¥è©¢
    "æœå°‹", "æŸ¥ä¸€ä¸‹", "æ‰¾ä¸€ä¸‹", "å¹«æˆ‘æŸ¥",  # æœå°‹è«‹æ±‚
    "æ€éº¼", "ç‚ºä»€éº¼", "ä»€éº¼æ˜¯", "å¦‚ä½•",    # çŸ¥è­˜å•ç­”
    "æ–°è", "è‚¡ç¥¨", "åŒ¯ç‡",                # è³‡è¨ŠæŸ¥è©¢
]

def should_use_bert(result: Dict, original_text: str = "") -> bool:
    """åˆ¤æ–·æ˜¯å¦å¯ä»¥ç›´æ¥ä½¿ç”¨ BERT çµæœ"""
    intent = result["intent"]
    confidence = result["confidence"]
    slots = result["slots"]
    
    # é—œéµå­—é éæ¿¾ï¼šåŒ…å«ç‰¹å®šé—œéµå­— â†’ äº¤çµ¦ LLM
    for keyword in LLM_KEYWORDS:
        if keyword in original_text:
            return False
    
    # ä½ä¿¡å¿ƒåº¦ â†’ äº¤çµ¦ LLM
    if confidence < BERT_CONFIDENCE_THRESHOLD:
        return False
    
    # ç´”èŠå¤© â†’ äº¤çµ¦ LLM
    if intent == "chat":
        return False
    
    # æª¢æŸ¥å¿…è¦ slots
    required_slots = {
        "turn_on": ["name"],
        "turn_off": ["name"],
        "get_state": ["name"],
        "climate_set_mode": ["mode"],
    }
    
    if intent in required_slots:
        for required_slot in required_slots[intent]:
            if required_slot not in slots or not slots[required_slot]:
                return False
    
    return True

def generate_catgirl_response(intent: str, slots: Dict) -> str:
    """ç‚º BERT çµæœç”Ÿæˆå–µåŒ–å›æ‡‰
    
    æ³¨æ„ï¼šå›æ‡‰ä¸å†åŒ…å«å€åŸŸåç¨±ï¼Œå› ç‚ºï¼š
    1. BERT å¯èƒ½éŒ¯èª¤æ‹†åˆ†ï¼ˆå¦‚ã€ŒåºŠé ­ç‡ˆã€â†’ area=åºŠ, name=åºŠé ­ç‡ˆï¼‰
    2. HA çµ„ä»¶æœƒå†æ¬¡ä¿®æ­£å€åŸŸ
    3. ä½¿ç”¨è€…é€šå¸¸ä¸éœ€è¦åœ¨å›æ‡‰ä¸­çœ‹åˆ°å€åŸŸç¢ºèª
    """
    name = slots.get("name", "è¨­å‚™")
    mode = slots.get("mode", "")
    
    # åªä½¿ç”¨ nameï¼Œä¸åŠ  areaï¼ˆé¿å…é‡è¤‡å’ŒéŒ¯èª¤ï¼‰
    templates = {
        "turn_on": [
            f"å¥½çš„å–µï¼æ­£åœ¨é–‹å•Ÿ{name}ï½",
            f"æ”¶åˆ°å–µï¼{name}é–‹èµ·ä¾†å›‰ï½",
            f"æ²’å•é¡Œå–µï¼å¹«ä½ é–‹å•Ÿ{name}äº†ï½",
        ],
        "turn_off": [
            f"å¥½çš„å–µï¼æ­£åœ¨é—œé–‰{name}ï½",
            f"æ”¶åˆ°å–µï¼{name}é—œæ‰å›‰ï½",
            f"æ²’å•é¡Œå–µï¼å¹«ä½ é—œé–‰{name}äº†ï½",
        ],
        "get_state": [
            f"è®“æˆ‘çœ‹çœ‹{name}çš„ç‹€æ…‹å–µï½",
            f"æˆ‘ä¾†æŸ¥ä¸€ä¸‹{name}å–µï¼",
        ],
        "climate_set_mode": [
            f"å¥½çš„å–µï¼æ­£åœ¨è¨­å®š{mode}æ¨¡å¼ï½",
            f"æ”¶åˆ°å–µï¼åˆ‡æ›åˆ°{mode}æ¨¡å¼å›‰ï½",
        ],
    }
    
    import random
    responses = templates.get(intent, [f"å¥½çš„å–µï¼"])
    return random.choice(responses)

def build_action_string(intent: str, slots: Dict, response_text: str) -> str:
    """å»ºæ§‹ ACTION å­—ä¸²ï¼ˆç”¨æ–¼æ­·å²è¨˜éŒ„ï¼‰"""
    lines = [response_text, f"ACTION {intent}"]
    
    for key, value in slots.items():
        lines.append(f"{key} {value}")
    
    return "\n".join(lines)

# ==============================================================================
# Qwen æ¨ç†ï¼ˆä¿æŒèˆ‡åŸç‰ˆç›¸åŒçš„é‚è¼¯ï¼‰
# ==============================================================================

SYSTEM_PROMPT = """ä½ æ˜¯æ—¥å’Œå–µï¼Œå¯æ„›çš„è²“å¨˜æ™ºæ…§å®¶å±…åŠ©ç†å–µï¼

è¼¸å‡ºæ ¼å¼ï¼š
<å›æ‡‰æ–‡å­—ï¼ˆåŠ å…¥ã€Œå–µã€å¢åŠ èŒæ„Ÿï¼‰>
ACTION <action_name>
name <è¨­å‚™ä¸­æ–‡åç¨±>
area <å€åŸŸä¸­æ–‡åç¨±>
[å…¶ä»–åƒæ•¸...]

å¯ç”¨ ACTIONï¼š
- turn_on/off: é–‹é—œè¨­å‚™ (name, area)
- get_state: æŸ¥è©¢è¨­å‚™ç‹€æ…‹ (name, area) - ç•¶ç”¨æˆ¶è©¢å•è¨­å‚™ç‹€æ…‹æ™‚å¿…é ˆä½¿ç”¨
- search: æœå°‹è³‡è¨Š (query) - ç•¶éœ€è¦æœå°‹å¤©æ°£ã€æ–°èã€ç¶²è·¯è³‡è¨Šæ™‚ä½¿ç”¨
- climate_set_mode: è¨­å®šç©ºèª¿æ¨¡å¼ (area, mode)

é‡è¦åŸå‰‡ï¼š
1. ä½ ä¸çŸ¥é“ä»»ä½•è¨­å‚™çš„ç•¶å‰ç‹€æ…‹ï¼Œå¿…é ˆä½¿ç”¨ get_state ä¾†æŸ¥è©¢
2. ç”¨æˆ¶è©¢å•ã€Œ...é–‹è‘—å—ã€ã€Œ...æ˜¯ä»€éº¼ç‹€æ…‹ã€æ™‚ï¼Œè¼¸å‡º ACTION get_state
3. ç”¨æˆ¶è©¢å•å¤©æ°£ã€æ–°èã€éœ€è¦ä¸Šç¶²æŸ¥è©¢çš„è³‡è¨Šæ™‚ï¼Œè¼¸å‡º ACTION search
4. name æ˜¯è¨­å‚™åç¨±ï¼ˆä¾‹å¦‚ï¼šå¤§ç‡ˆã€é¢¨æ‰‡ã€å†·æ°£ï¼‰
5. area æ˜¯å€åŸŸåç¨±ï¼ˆä¾‹å¦‚ï¼šæ›¸æˆ¿ã€å®¢å»³ã€è‡¥å®¤ï¼‰ï¼Œé è¨­ç”¨æ›¸æˆ¿
6. ç´”èŠå¤©æ™‚ä¸è¼¸å‡º ACTION
7. å›æ‡‰è¦è¦ªåˆ‡å¯æ„›ï¼Œé©ç•¶åŠ å…¥ã€Œå–µã€

ç¯„ä¾‹ï¼š
ç”¨æˆ¶ï¼šé—œæ‰æ›¸æˆ¿å¤§ç‡ˆ
åŠ©ç†ï¼šå¥½çš„ï¼Œæ­£åœ¨é—œé–‰æ›¸æˆ¿å¤§ç‡ˆå–µ
ACTION turn_off
name å¤§ç‡ˆ
area æ›¸æˆ¿

ç”¨æˆ¶ï¼šæ˜å¤©æœƒä¸‹é›¨å—
åŠ©ç†ï¼šè®“æˆ‘æŸ¥ä¸€ä¸‹å¤©æ°£é å ±å–µï½
ACTION search
query æ˜å¤©å¤©æ°£é å ±

ç”¨æˆ¶ï¼šä¸­å’Œå€ä»Šå¤©å¤©æ°£å¦‚ä½•
åŠ©ç†ï¼šæˆ‘ä¾†æŸ¥ä¸€ä¸‹ä¸­å’Œå€çš„å¤©æ°£å–µï¼
ACTION search
query ä¸­å’Œå€ä»Šå¤©å¤©æ°£"""

def qwen_inference(user_input: str, history: list = None) -> str:
    """ä½¿ç”¨ Qwen é€²è¡Œæ¨ç†"""
    current_dt = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "system", "content": f"ç¾åœ¨æ™‚é–“: {current_dt}"},
    ]
    
    if history:
        for msg in history:
            messages.append({"role": msg["role"], "content": msg["content"]})
    
    messages.append({"role": "user", "content": f"User request:\n{user_input}"})
    
    prompt = qwen_tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = qwen_tokenizer(
        prompt, 
        return_tensors="pt",
        max_length=MAX_SEQ_LENGTH,
        truncation=True
    ).to("cuda")
    
    outputs = qwen_model.generate(
        **inputs,
        max_new_tokens=150,
        temperature=0.3,
        top_p=0.95,
        do_sample=True,
        pad_token_id=qwen_tokenizer.eos_token_id,
    )
    
    # ä¸è·³éç‰¹æ®Š tokenï¼Œé€™æ¨£æ‰èƒ½æ­£ç¢ºåˆ†å‰²
    response = qwen_tokenizer.decode(outputs[0], skip_special_tokens=False)
    
    # æå–æœ€å¾Œä¸€å€‹åŠ©ç†å›æ‡‰
    # Qwen æ ¼å¼: <|im_start|>assistant\n{å›æ‡‰}<|im_end|>
    if "<|im_start|>assistant" in response:
        # å–å¾—æœ€å¾Œä¸€å€‹ assistant å€å¡Š
        parts = response.split("<|im_start|>assistant")
        last_response = parts[-1]
        
        # ç§»é™¤çµå°¾æ¨™è¨˜
        if "<|im_end|>" in last_response:
            last_response = last_response.split("<|im_end|>")[0]
        
        # ç§»é™¤é–‹é ­çš„æ›è¡Œ
        response = last_response.strip()
    else:
        # å‚™ç”¨ï¼šå˜—è©¦å…¶ä»–æ ¼å¼
        if "assistant\n" in response:
            parts = response.split("assistant\n")
            response = parts[-1].strip()
    
    # ç°¡è½‰ç¹
    response = s2t_converter.convert(response)
    
    return response

def parse_action_from_response(response: str) -> Optional[Dict]:
    """å¾ Qwen å›æ‡‰è§£æ ACTION"""
    lines = response.strip().split('\n')
    action = None
    params = {}
    
    for line in lines:
        line = line.strip()
        
        if line.startswith('ACTION '):
            action = line[7:].strip()
            continue
        
        for param in ['name', 'area', 'mode', 'temperature', 'brightness', 'query']:
            if line.startswith(f'{param} '):
                params[param] = line[len(param)+1:].strip()
                break
    
    if action:
        return {"action": action, "params": params}
    return None

# ==============================================================================
# API Endpoints
# ==============================================================================

@app.on_event("startup")
async def startup():
    """å•Ÿå‹•æ™‚è¼‰å…¥æ¨¡å‹"""
    load_bert_model()
    load_qwen_model()

@app.get("/")
def root():
    return {
        "service": "Qwen + BERT Catgirl Home Assistant API",
        "bert_model": BERT_MODEL_PATH,
        "bert_confidence_threshold": BERT_CONFIDENCE_THRESHOLD,
    }

@app.get("/health")
def health():
    return {"status": "healthy", "bert_loaded": bert_model is not None, "qwen_loaded": qwen_model is not None}

@app.post("/process", response_model=ActionResult)
async def process_request(request: InferenceRequest):
    """
    è™•ç†ç”¨æˆ¶è«‹æ±‚
    
    æµç¨‹ï¼š
    1. BERT æ„åœ–åˆ†é¡ + å¡«æ§½
    2. é«˜ç¢ºå®šæ€§ â†’ ç›´æ¥è¿”å› ACTION
    3. ä½ç¢ºå®šæ€§/èŠå¤© â†’ Qwen LLM è™•ç†
    """
    logger.info("=" * 60)
    logger.info(f"ğŸ“¥ æ”¶åˆ°è«‹æ±‚: {request.text}")
    
    try:
        # ===== Step 0: ç°¡å–®æ¨¡æ¿å›è¦†ï¼ˆæœ€å¿«ï¼Œä¸éœ€è¦ä»»ä½•æ¨¡å‹ï¼‰=====
        
        # æ™‚é–“è©¢å•
        TIME_PATTERN = re.compile(r'(å¹¾é»å¹¾åˆ†|ç¾åœ¨å¹¾é»|ç¾åœ¨æ˜¯å¹¾é»|ç¾åœ¨å¹¾é»å¹¾åˆ†|å¹¾é»äº†)')
        if TIME_PATTERN.search(request.text):
            now = datetime.now()
            hour = now.hour
            minute = now.minute
            period = 'ä¸Šåˆ' if hour < 12 else 'ä¸‹åˆ'
            hour12 = hour if 1 <= hour <= 12 else (hour - 12 if hour > 12 else 12)
            response_text = f"ç¾åœ¨æ˜¯{period}{hour12}é»{minute:02d}åˆ†å–µï½"
            logger.info(f"â° æ™‚é–“è©¢å•ï¼Œæ¨¡æ¿å›è¦†: {response_text}")
            
            return ActionResult(
                action=None,
                params={},
                response_text=response_text,
                has_action=False,
                raw_response=response_text,
                processed_by="template",
            )
        
        # æ—¥æœŸè©¢å•
        DATE_PATTERN = re.compile(r'(å¹¾æœˆå¹¾è™Ÿ|ä»Šå¤©å¹¾è™Ÿ|ç¾åœ¨æ˜¯å¹¾è™Ÿ|ä»Šå¤©æ˜¯å¹¾æœˆå¹¾è™Ÿ|ä»Šå¤©æ—¥æœŸ|ä»Šå¤©æ˜ŸæœŸå¹¾)')
        if DATE_PATTERN.search(request.text):
            now = datetime.now()
            weekdays = ['ä¸€', 'äºŒ', 'ä¸‰', 'å››', 'äº”', 'å…­', 'æ—¥']
            weekday = weekdays[now.weekday()]
            response_text = f"ä»Šå¤©æ˜¯{now.year}å¹´{now.month}æœˆ{now.day}æ—¥ï¼Œæ˜ŸæœŸ{weekday}å–µï½"
            logger.info(f"ğŸ“… æ—¥æœŸè©¢å•ï¼Œæ¨¡æ¿å›è¦†: {response_text}")
            
            return ActionResult(
                action=None,
                params={},
                response_text=response_text,
                has_action=False,
                raw_response=response_text,
                processed_by="template",
            )
        
        # å¤©æ°£æŸ¥è©¢ â†’ ç›´æ¥æ§‹é€  search ACTION
        WEATHER_PATTERN = re.compile(r'(å¤©æ°£|ä¸‹é›¨|æ°£æº«|æœƒä¸æœƒä¸‹é›¨|é™é›¨)')
        if WEATHER_PATTERN.search(request.text):
            # æå–æŸ¥è©¢å…§å®¹ï¼ˆç›´æ¥ç”¨åŸå§‹è¼¸å…¥ä½œç‚º queryï¼‰
            query = request.text
            response_text = f"è®“æˆ‘æŸ¥ä¸€ä¸‹å¤©æ°£è³‡è¨Šå–µï½"
            logger.info(f"ğŸŒ¤ï¸ å¤©æ°£æŸ¥è©¢ï¼Œç›´æ¥æ§‹é€  search ACTION")
            
            return ActionResult(
                action="search",
                params={"query": query},
                response_text=response_text,
                has_action=True,
                raw_response=f"{response_text}\nACTION search\nquery {query}",
                processed_by="template",
            )
        
        # ===== Step 1: BERT é è™•ç† =====
        bert_result = bert_predict(request.text)
        logger.info(f"ğŸ” BERT: intent={bert_result['intent']}, conf={bert_result['confidence']:.2%}, slots={bert_result['slots']}")
        
        # Step 2: åˆ¤æ–·æ˜¯å¦å¯ä»¥ç›´æ¥ä½¿ç”¨ BERT çµæœ
        if should_use_bert(bert_result, request.text):
            logger.info("âš¡ BERT ç›´æ¥è™•ç†ï¼ˆé«˜ç¢ºå®šæ€§ï¼‰")
            
            intent = bert_result["intent"]
            slots = bert_result["slots"]
            
            # ç”Ÿæˆå–µåŒ–å›æ‡‰
            response_text = generate_catgirl_response(intent, slots)
            raw_response = build_action_string(intent, slots, response_text)
            
            logger.info(f"ğŸ“¤ BERT å›æ‡‰: {response_text}")
            
            return ActionResult(
                action=intent,
                params=slots,
                response_text=response_text,
                has_action=True,
                raw_response=raw_response,
                processed_by="bert",
            )
        
        # Step 3: äº¤çµ¦ Qwen LLM è™•ç†
        logger.info("ğŸ§  äº¤çµ¦ Qwen LLM è™•ç†...")
        
        response = qwen_inference(request.text, request.history)
        action_data = parse_action_from_response(response)
        
        # æå–ç´”æ–‡å­—å›æ‡‰
        response_lines = response.split('\n')
        response_text_lines = []
        for line in response_lines:
            if not line.strip().startswith(('ACTION', 'name', 'area', 'mode', 'temperature', 'brightness', 'query')):
                response_text_lines.append(line)
        response_text = '\n'.join(response_text_lines).strip()
        
        logger.info(f"ğŸ“¤ Qwen å›æ‡‰: {response_text}")
        
        return ActionResult(
            action=action_data['action'] if action_data else None,
            params=action_data['params'] if action_data else {},
            response_text=response_text,
            has_action=action_data is not None,
            raw_response=response,
            processed_by="qwen",
        )
        
    except Exception as e:
        import traceback
        logger.error(f"âŒ è™•ç†è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/process_with_state", response_model=SearchResultResponse)
async def process_with_state(request: StateResultRequest):
    """è™•ç† get_state äºŒæ¬¡å°è©±ï¼ˆä½¿ç”¨ LLM ç”Ÿæˆå›æ‡‰ï¼‰"""
    logger.info(f"ğŸ” get_state äºŒæ¬¡å°è©±: {request.device_name}@{request.area}")
    
    full_name = f"{request.area}{request.device_name}" if request.area else request.device_name
    
    # æ§‹å»ºå¤šè¼ªå°è©± prompt
    current_dt = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "system", "content": f"ç¾åœ¨æ™‚é–“: {current_dt}"},
        {"role": "user", "content": f"User request:\n{request.original_question}"},
        {"role": "assistant", "content": f"æˆ‘ä¾†å¹«ä½ çœ‹ä¸€ä¸‹å–µï½\nACTION get_state\nname {request.device_name}\narea {request.area}"},
        {"role": "user", "content": f"State result:\n{full_name}: {request.state_result}"},
    ]
    
    prompt = qwen_tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = qwen_tokenizer(
        prompt, 
        return_tensors="pt",
        max_length=MAX_SEQ_LENGTH,
        truncation=True
    ).to("cuda")
    
    outputs = qwen_model.generate(
        **inputs,
        max_new_tokens=100,
        temperature=0.5,  # ç¨é«˜çš„æº«åº¦å¢åŠ å¤šæ¨£æ€§
        top_p=0.9,
        do_sample=True,
        pad_token_id=qwen_tokenizer.eos_token_id
    )
    
    response = qwen_tokenizer.decode(outputs[0], skip_special_tokens=False)
    
    # æå–æœ€å¾Œä¸€å€‹ assistant å›æ‡‰
    if "<|im_start|>assistant" in response:
        parts = response.split("<|im_start|>assistant")
        last_part = parts[-1].strip()
        if last_part.startswith("\n"):
            last_part = last_part[1:]
        if "<|im_end|>" in last_part:
            last_part = last_part.split("<|im_end|>")[0]
        response = last_part.strip()
    
    # æ¸…ç†
    response = response.replace("<|im_end|>", "").replace("<|im_start|>", "").strip()
    response = s2t_converter.convert(response)
    
    logger.info(f"ğŸ“¤ äºŒæ¬¡å°è©±å›æ‡‰: {response}")
    
    return SearchResultResponse(response_text=response)

@app.post("/search_result", response_model=SearchResultResponse)
async def process_search_result(request: SearchResultRequest):
    """è™•ç†æœå°‹çµæœå›æ‡‰"""
    # ä½¿ç”¨ Qwen å–µåŒ–æœå°‹çµæœ
    # é€™è£¡å¯ä»¥ç›´æ¥ç”¨æ¨¡æ¿æˆ–èª¿ç”¨ Qwen
    response_text = f"{request.search_result}å–µï½"
    return SearchResultResponse(response_text=response_text)

@app.post("/fallback_assistant", response_model=SearchResultResponse)
async def process_fallback_assistant(request: FallbackAssistantRequest):
    """è™•ç† Fallback åŠ©ç†å›æ‡‰å–µåŒ–"""
    # ç°¡å–®å–µåŒ–
    response_text = request.assistant_response
    if "å–µ" not in response_text:
        response_text = f"{response_text}å–µï½"
    return SearchResultResponse(response_text=response_text)

# ==============================================================================
# ä¸»ç¨‹åº
# ==============================================================================
if __name__ == "__main__":
    logger.info("ğŸš€ å•Ÿå‹• Qwen + BERT Catgirl Home Assistant API Server")
    logger.info(f"ğŸ“¡ ç›£è½: {HOST}:{PORT}")
    logger.info(f"ğŸ”§ BERT ä¿¡å¿ƒé–¾å€¼: {BERT_CONFIDENCE_THRESHOLD}")
    uvicorn.run(app, host=HOST, port=PORT)
