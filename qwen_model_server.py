#!/usr/bin/env python3
"""
qwen_model_server.py
æä¾› Qwen æ¨¡å‹çš„ REST API æœå‹™ (v6 - name + area æ ¼å¼)
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
from unsloth import FastLanguageModel
import uvicorn
from typing import Optional, List, Dict
import re
from collections import defaultdict
import logging
from datetime import datetime
from opencc import OpenCC




# ==============================================================================
# é…ç½®
# å¯ä»¥é¸æ“‡"./qwen-catgirl-ha-switch-v2"
# æˆ–"./qwen2.5-1.5b-home-assistant"
# ==============================================================================
MODEL_PATH = "./qwen-catgirl-ha-switch-v2"
MAX_SEQ_LENGTH = 2048
HOST = "0.0.0.0"
PORT = 8124  # æ”¹æˆ 8124 é¿å…èˆ‡ HA è¡çª

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    force=True  # å¼·åˆ¶è¦†è“‹ç¾æœ‰çš„æ—¥èªŒé…ç½®
)
logger = logging.getLogger(__name__)

# åŒæ™‚è¨­å®š uvicorn çš„ logger
uvicorn_logger = logging.getLogger("uvicorn")
uvicorn_access_logger = logging.getLogger("uvicorn.access")

app = FastAPI(title="Qwen Catgirl Home Assistant API v6")

# ==============================================================================
# å…¨å±€è®Šæ•¸
# ==============================================================================
model = None
tokenizer = None
s2t_converter = OpenCC('s2t')  # ç°¡é«”è½‰ç¹é«”

# èŠå¤©å›æ‡‰è©å½™ä¿®æ­£å­—å…¸ï¼ˆä¿®æ­£é«˜éš¨æ©Ÿæ€§å°è‡´çš„ä¸ç•¶ç”¨è©ï¼‰
CHAT_WORD_FIXES = {
    r'å¤¢é­˜': 'å¤¢å¢ƒ',
    r'å™©å¤¢': 'å¥½å¤¢',
}

# ==============================================================================
# System Prompt (v7 ç‰ˆæœ¬ - ä¸é å…ˆçµ¦è¨­å‚™åˆ—è¡¨)
# ==============================================================================
current_dt = datetime.now().strftime("%Y-%m-%d %H:%M")
current_time = datetime.now().strftime("%H:%M")
SYSTEM_PROMPT = """ä½ æ˜¯æ—¥å’Œå–µï¼Œå¯æ„›çš„è²“å¨˜æ™ºæ…§å®¶å±…åŠ©ç†å–µï¼

è¼¸å‡ºæ ¼å¼ï¼š
<å›æ‡‰æ–‡å­—ï¼ˆåŠ å…¥ã€Œå–µã€å¢åŠ èŒæ„Ÿï¼‰>
ACTION <action_name>
name <è¨­å‚™ä¸­æ–‡åç¨±>
area <å€åŸŸä¸­æ–‡åç¨±>
[å…¶ä»–åƒæ•¸...]

å¯ç”¨ ACTIONï¼š
- turn_on/off: é–‹é—œè¨­å‚™ (name, area)
- get_state: æŸ¥è©¢è¨­å‚™ç‹€æ…‹ (name, area) - ç•¶ç”¨æˆ¶è©¢å•è¨­å‚™ç‹€æ…‹æ™‚å¿…é ˆä½¿ç”¨
- search: æœå°‹è³‡è¨Š (query) 
- climate_set_mode: è¨­å®šç©ºèª¿æ¨¡å¼ (area, mode)

é‡è¦åŸå‰‡ï¼š
1. ä½ ä¸çŸ¥é“ä»»ä½•è¨­å‚™çš„ç•¶å‰ç‹€æ…‹ï¼Œå¿…é ˆä½¿ç”¨ get_state ä¾†æŸ¥è©¢
2. ç”¨æˆ¶è©¢å•ã€Œ...é–‹è‘—å—ã€ã€Œ...æ˜¯ä»€éº¼ç‹€æ…‹ã€ã€Œç¾åœ¨å¹¾åº¦ã€ç­‰å•é¡Œæ™‚ï¼Œå¿…é ˆè¼¸å‡º ACTION get_state
3. åªè¼¸å‡ºä¸­æ–‡ name å’Œ areaï¼Œä¸è¦è¼¸å‡º entity_id
4. name æ˜¯è¨­å‚™åç¨±ï¼ˆä¾‹å¦‚ï¼šå¤§ç‡ˆã€é¢¨æ‰‡ã€å†·æ°£ï¼‰
5. area æ˜¯å€åŸŸåç¨±ï¼ˆä¾‹å¦‚ï¼šæ›¸æˆ¿ã€å®¢å»³ã€è‡¥å®¤ï¼‰ï¼Œå¦‚æœä½¿ç”¨è€…æ²’æœ‰èªªå€åŸŸçš„è©±ï¼Œé è¨­ç”¨æ›¸æˆ¿
6. ç´”èŠå¤©æ™‚ä¸è¼¸å‡º ACTION
7. å›æ‡‰è¦è¦ªåˆ‡å¯æ„›ï¼Œé©ç•¶åŠ å…¥ã€Œå–µã€

ç¯„ä¾‹ï¼š
ç”¨æˆ¶ï¼šå®¢å»³ç‡ˆé–‹è‘—å—
åŠ©ç†ï¼šæˆ‘ä¾†å¹«ä½ çœ‹ä¸€ä¸‹å–µï½
ACTION get_state
name ç‡ˆ
area å®¢å»³

ç”¨æˆ¶ï¼šé—œæ‰æ›¸æˆ¿å¤§ç‡ˆ
åŠ©ç†ï¼šå¥½çš„ï¼Œæ­£åœ¨é—œé–‰æ›¸æˆ¿å¤§ç‡ˆå–µ
ACTION turn_off
name å¤§ç‡ˆ
area æ›¸æˆ¿

ç”¨æˆ¶ï¼šè¨­å®šå®¢å»³å†·æ°£ç‚ºå†·æ°£æ¨¡å¼  
åŠ©ç†ï¼šå¥½çš„å–µï¼æ­£åœ¨è¨­å®šå®¢å»³å†·æ°£ç‚ºå†·æ°£æ¨¡å¼å–µ
ACTION climate_set_mode
area å®¢å»³
mode cool

ç”¨æˆ¶ï¼šä½ å¥½
åŠ©ç†ï¼šä½ å¥½å‘€ä¸»äººï¼æœ‰ä»€éº¼éœ€è¦å¹«å¿™çš„å—å–µï¼Ÿ"""
# ==============================================================================
# æœå°‹çµæœå›ç­”ä¹‹ç°¡æ˜“æç¤ºè©
# ==============================================================================
SEARCH_RESULT_PROMPT = """ä½ æ˜¯æ—¥å’Œå–µï¼Œå¯æ„›çš„è²“å¨˜æ™ºæ…§å®¶å±…åŠ©ç†å–µï¼

ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šæœå°‹çµæœï¼Œç”¨è²“å¨˜é¢¨æ ¼å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

ã€é‡è¦åŸå‰‡ã€‘
1. **çµ•å°ä¸èƒ½æ”¹è®Šä»»ä½•æ•¸å­—ã€æ™‚é–“ã€æ—¥æœŸã€æº«åº¦ã€ç™¾åˆ†æ¯”ç­‰å…·é«”è³‡è¨Š**
2. ä¿ç•™æœå°‹çµæœä¸­çš„æ‰€æœ‰é‡è¦è³‡è¨Šï¼ˆé™é›¨æ©Ÿç‡ã€æº«åº¦ã€æ¿•åº¦ç­‰ï¼‰
3. ç”¨è¦ªåˆ‡å¯æ„›çš„èªæ°£è¡¨é”ï¼ŒåŠ å…¥ã€Œå–µã€å¢åŠ èŒæ„Ÿï¼ˆ1-2å€‹å–µå³å¯ï¼‰
4. **ç›´æ¥è½‰è¿°è³‡è¨Šï¼Œä¸è¦é¡å¤–è§£ææˆ–ç¸½çµ**
5. ä¿æŒç°¡æ½”ï¼Œä¸è¦éåº¦è§£é‡‹
6. ä¸è¦æ·»åŠ æœå°‹çµæœä¸­æ²’æœ‰çš„è³‡è¨Šï¼ˆå¦‚ã€Œé©å®œå¤–å‡ºã€ç­‰å»ºè­°ï¼‰
7. é¿å…ä½¿ç”¨é …ç›®ç¬¦è™Ÿæˆ–è¤‡é›œæ ¼å¼ï¼Œç”¨è‡ªç„¶èªå¥è¡¨é”

ã€ç¯„ä¾‹ã€‘
å•é¡Œï¼šæ˜å¤©æœƒä¸‹é›¨å—
æœå°‹çµæœï¼šæ˜å¤©æœ‰60%çš„é™é›¨æ©Ÿç‡ï¼Œæº«åº¦25åº¦
å›ç­”ï¼šæ˜å¤©æœ‰60%çš„é™é›¨æ©Ÿç‡ï¼Œæº«åº¦25åº¦å–”å–µï½è¨˜å¾—å¸¶å‚˜å–µï¼

å•é¡Œï¼šå°åŒ—ç¾åœ¨å¹¾åº¦
æœå°‹çµæœï¼šå°åŒ—ç¾åœ¨æ°£æº«28åº¦ï¼Œæ¿•åº¦75%
å›ç­”ï¼šå°åŒ—ç¾åœ¨æ°£æº«28åº¦ï¼Œæ¿•åº¦75%å–”å–µï½

å•é¡Œï¼šä¸­å’Œå€ä»Šå¤©çš„å¤©æ°£
æœå°‹çµæœï¼šæ ¹æ“šAccuWeatherçš„è³‡æ–™ï¼Œä»Šæ™šä¸­å’Œå€æ™šä¸Š6é»åˆ°11é»çš„æº«åº¦å¤§ç´„åœ¨72-75Â°F (ç´„22-24Â°C) ä¹‹é–“ã€‚ä¸­å¤®æ°£è±¡ç½²çš„è³‡æ–™é¡¯ç¤ºï¼Œæ–°åŒ—å¸‚ä»Šæ™šé™é›¨æ©Ÿç‡ç‚º10%
å›ç­”ï¼šæ ¹æ“šAccuWeatherï¼Œä»Šæ™šä¸­å’Œå€æ™šä¸Š6é»åˆ°11é»çš„æº«åº¦å¤§ç´„åœ¨72-75Â°F (ç´„22-24Â°C) ä¹‹é–“å–µï½ä¸­å¤®æ°£è±¡ç½²èªªæ–°åŒ—å¸‚ä»Šæ™šé™é›¨æ©Ÿç‡ç‚º10%å–”ï¼"""

# ==============================================================================
# Fallback åŠ©ç†å›æ‡‰å–µåŒ–æç¤ºè©
# ==============================================================================
FALLBACK_ASSISTANT_PROMPT = """ä½ æ˜¯æ—¥å’Œå–µï¼Œå¯æ„›çš„è²“å¨˜æ™ºæ…§å®¶å±…åŠ©ç†å–µï¼

ä½ çš„ä»»å‹™æ˜¯å°‡å…¶ä»–åŠ©ç†ï¼ˆå¦‚ Geminiã€Home Assistant Assistï¼‰çš„å›æ‡‰ï¼Œç”¨ä½ ç¨ç‰¹çš„è²“å¨˜é¢¨æ ¼é‡æ–°è¡¨é”ã€‚

ã€é‡è¦åŸå‰‡ã€‘
1. **çµ•å°ä¸èƒ½æ”¹è®Šä»»ä½•æ•¸å­—ã€æ™‚é–“ã€æ—¥æœŸã€æº«åº¦ç­‰å…·é«”è³‡è¨Š**
2. ä¿ç•™åŸå§‹å›æ‡‰çš„å®Œæ•´å…§å®¹å’Œæ„æ€ï¼Œä¸€å­—ä¸æ¼
3. åªèª¿æ•´èªæ°£ï¼ŒåŠ å…¥ã€Œå–µã€å¢åŠ èŒæ„Ÿï¼ˆä½†ä¸è¦éåº¦ä½¿ç”¨ï¼Œ1-2å€‹å–µå³å¯ï¼‰
4. ä½¿ç”¨è¦ªåˆ‡å¯æ„›çš„èªæ°£
5. ä¿æŒç°¡çŸ­è‡ªç„¶
6. ä¸è¦æ·»åŠ åŸæœ¬æ²’æœ‰çš„è³‡è¨Š
7. ä¸è¦çœç•¥ä»»ä½•é‡è¦è³‡è¨Š

ã€ç¯„ä¾‹ã€‘
åŸå§‹ï¼šæº«åº¦å·²è¨­å®š27
å–µåŒ–ï¼šå¥½çš„å–µï¼æº«åº¦å·²ç¶“è¨­å®šç‚º27åº¦äº†å–µï½

åŸå§‹ï¼šæé†’å·²è¨­å®šåœ¨æ˜å¤©ä¸‹åˆ3é»
å–µåŒ–ï¼šå¥½çš„å–µï¼å·²ç¶“å¹«ä½ è¨­å®šæ˜å¤©ä¸‹åˆ3é»çš„æé†’äº†ï½

åŸå§‹ï¼šå·²é–‹å•Ÿå®¢å»³çš„ç‡ˆ
å–µåŒ–ï¼šå¥½çš„å–µï¼å·²ç¶“å¹«ä½ é–‹å•Ÿå®¢å»³çš„ç‡ˆäº†ï½"""

# ==============================================================================
# Request/Response Models
# ==============================================================================
class Device(BaseModel):
    entityId: str
    friendlyName: str
    domain: str
    state: str
    area: Optional[str] = None  # æ–°å¢ area æ¬„ä½
    brightnessPct: Optional[int] = None
    color: Optional[str] = None
    currentTemp: Optional[float] = None
    targetTemp: Optional[float] = None
    position: Optional[int] = None
    percentage: Optional[int] = None

class InferenceRequest(BaseModel):
    """è™•ç†ç”¨æˆ¶è«‹æ±‚ï¼ˆv7: devices è®Šæˆå¯é¸ï¼‰"""
    text: str
    devices: Optional[List[Device]] = None  # v7: è®Šæˆå¯é¸ï¼Œä¸å†å¼·åˆ¶è¦æ±‚
    language: Optional[str] = "zh"
    history: Optional[List[dict]] = None  # å°è©±æ­·å²ï¼š[{"role": "user", "content": "..."}, ...]

class StateResultRequest(BaseModel):
    """v7: è™•ç† get_state äºŒæ¬¡å°è©±è«‹æ±‚"""
    original_question: str  # ç”¨æˆ¶çš„åŸå§‹å•é¡Œ
    state_result: str  # get_state è¿”å›çš„ç‹€æ…‹çµæœ
    device_name: str  # è¨­å‚™åç¨±
    area: str  # å€åŸŸåç¨±

class SearchResultRequest(BaseModel):
    """æœå°‹çµæœå›æ‡‰è«‹æ±‚ï¼ˆä¸éœ€è¦è£ç½®åˆ—è¡¨ï¼‰"""
    user_question: str  # ä½¿ç”¨è€…çš„åŸå§‹å•é¡Œ
    search_result: str  # æœå°‹å·¥å…·è¿”å›çš„çµæœ

class FallbackAssistantRequest(BaseModel):
    """Fallback åŠ©ç†å›æ‡‰å–µåŒ–è«‹æ±‚"""
    user_question: str  # ä½¿ç”¨è€…çš„åŸå§‹å•é¡Œ
    assistant_response: str  # å…¶ä»–åŠ©ç†ï¼ˆGoogle Assistantã€Alexaç­‰ï¼‰çš„å›æ‡‰

class ActionResult(BaseModel):
    action: Optional[str] = None
    params: Dict[str, str] = {}
    response_text: str
    has_action: bool
    raw_response: Optional[str] = None  # ä¿å­˜åŒ…å« ACTION çš„åŸå§‹å›æ‡‰ï¼Œç”¨æ–¼å°è©±æ­·å²

class SearchResultResponse(BaseModel):
    """æœå°‹çµæœå›æ‡‰ï¼ˆåªåŒ…å«æ–‡å­—å›æ‡‰ï¼‰"""
    response_text: str

# ==============================================================================
# è¼‰å…¥æ¨¡å‹
# ==============================================================================
@app.on_event("startup")
async def load_model():
    global model, tokenizer
    logger.info("=" * 80)
    logger.info("ğŸš€ æ­£åœ¨è¼‰å…¥ Qwen v6 æ¨¡å‹ (name + area æ ¼å¼)...")
    logger.info(f"ğŸ“‚ æ¨¡å‹è·¯å¾‘: {MODEL_PATH}")
    logger.info("=" * 80)
    
    # æ ¹æ“š MODEL_PATH è‡ªå‹•ç‚º 1.5B æ¨¡å‹é¸æ“‡å…¨ç²¾åº¦æ¨ç†
    if "1.5b" in MODEL_PATH.lower():
        logger.info("ğŸ”§ åµæ¸¬åˆ° 1.5B æ¨¡å‹ï¼Œå•Ÿç”¨å…¨ç²¾åº¦ (float32) æ¨ç†ï¼Œé—œé–‰ 4-bit åŠ é€Ÿ")
        dtype = torch.float32
        load_in_4bit = False
    else:
        logger.info("ğŸ”§ æœªåµæ¸¬åˆ° 1.5Bï¼Œä½¿ç”¨é è¨­é‡åŒ–/çœè¨˜æ†¶é«”è¨­å®šï¼ˆ4-bitï¼‰")
        dtype = None
        load_in_4bit = True
    
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=MODEL_PATH,
        max_seq_length=MAX_SEQ_LENGTH,
        dtype=dtype,
        load_in_4bit=load_in_4bit,
    )
    
    FastLanguageModel.for_inference(model)
    
    logger.info("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼")
    logger.info(f"ğŸ“Š max_seq_length: {MAX_SEQ_LENGTH}")
    logger.info(f"ğŸ“¡ ç›£è½ç«¯å£: {PORT}")
    logger.info("=" * 80)

# ==============================================================================
# Device List æ ¼å¼åŒ–ï¼ˆDomain åˆ†çµ„ + areaï¼‰
# ==============================================================================
def devices_to_domain_grouped_format(devices: List[Device]) -> str:
    """å°‡è¨­å‚™åˆ—è¡¨è½‰æ›ç‚º Domain åˆ†çµ„æ ¼å¼ï¼ˆv6 - åŒ…å« areaï¼‰"""
    grouped = defaultdict(list)
    
    for device in devices:
        grouped[device.domain].append(device)
    
    lines = []
    for domain in sorted(grouped.keys()):
        lines.append(f"{domain}:")
        for device in grouped[domain]:
            short_id = device.entityId.split('.', 1)[1] if '.' in device.entityId else device.entityId
            name = device.friendlyName
            state = device.state
            
            # v6: åŠ å…¥ area è³‡è¨Š
            area = f"[{device.area}]" if device.area else ""
            
            line = f"  {short_id} '{name}' {area} {state}"
            
            # æ·»åŠ é¡å¤–å±¬æ€§
            if device.domain == 'light' and device.brightnessPct:
                line += f" {device.brightnessPct}%"
                if device.color:
                    line += f" {device.color}"
            elif device.domain == 'climate':
                if device.currentTemp:
                    line += f" curr={device.currentTemp}"
                if device.targetTemp:
                    line += f" target={device.targetTemp}"
            elif device.domain == 'cover' and device.position is not None:
                line += f" pos={device.position}%"
            elif device.domain == 'fan' and device.percentage:
                line += f" {device.percentage}%"
            
            lines.append(line)
    
    return '\n'.join(lines)

# ==============================================================================
# ACTION è§£æ (v6 æ ¼å¼ï¼šæ”¯æ´ name + area)
# ==============================================================================
def parse_action_from_response(response: str) -> Optional[Dict[str, any]]:
    """å¾æ¨¡å‹è¼¸å‡ºè§£æ ACTION (v6 æ ¼å¼)"""
    if "ACTION" not in response:
        return None
    
    lines = response.split('\n')
    action = None
    params = {}
    
    for line in lines:
        line = line.strip()
        
        # æå– ACTION
        if line.startswith("ACTION "):
            action = line.replace("ACTION ", "").strip()
        
        # v6: æå– nameï¼ˆä¸å¸¶å¼•è™Ÿï¼‰
        elif line.startswith("name "):
            params['name'] = line.replace("name ", "").strip()
        
        # v6: æå– areaï¼ˆä¸å¸¶å¼•è™Ÿï¼‰
        elif line.startswith("area "):
            params['area'] = line.replace("area ", "").strip()
        
        # æå–å…¶ä»–åƒæ•¸
        elif line.startswith("brightness "):
            params['brightness'] = int(line.replace("brightness ", "").strip())
        
        elif line.startswith("color "):
            params['color'] = line.replace("color ", "").strip()
        
        elif line.startswith("temperature "):
            params['temperature'] = float(line.replace("temperature ", "").strip())
        
        elif line.startswith("command "):
            params['command'] = line.replace("command ", "").strip()
        
        elif line.startswith("position "):
            params['position'] = int(line.replace("position ", "").strip())
        
        elif line.startswith("volume "):
            params['volume'] = int(line.replace("volume ", "").strip())
        elif line.startswith("query "):
            params['query'] = line.replace("query ", "").strip()
        elif line.startswith('mode '):  # â­ åŠ å…¥é€™è¡Œï¼
            params['mode'] = line.replace('mode ', '').strip()
    if not action:
        return None
    
    return {
        "action": action,
        "params": params
    }

# ==============================================================================
# æ¨ç†
# ==============================================================================
# æ§åˆ¶é—œéµå­—æ­£å‰‡è¡¨é”å¼
CONTROL_KEYWORDS_PATTERN = re.compile(
    r'(æº«åº¦|æ¿•åº¦|é–‹|é—œ|æŠŠ|è¨­å®š|èª¿æ•´|æ‰“é–‹|é—œé–‰|åˆ‡æ›|èª¿æˆ|è®Šæˆ|è¨­ç‚º|å•Ÿå‹•|åœæ­¢|æ”¹æˆ)'
)

# æ™‚é–“/æ—¥æœŸè©¢å•åµæ¸¬
TIME_QUESTION_PATTERN = re.compile(r'(å¹¾é»å¹¾åˆ†|ç¾åœ¨å¹¾é»|ç¾åœ¨æ˜¯å¹¾é»|ç¾åœ¨å¹¾é»å¹¾åˆ†)')
DATE_QUESTION_PATTERN = re.compile(r'(å¹¾æœˆå¹¾è™Ÿ|ä»Šå¤©å¹¾è™Ÿ|ç¾åœ¨æ˜¯å¹¾è™Ÿ|ä»Šå¤©æ˜¯å¹¾æœˆå¹¾è™Ÿ|ä»Šå¤©æ—¥æœŸ)')


def run_inference(user_input: str, device_list: str = None, history: list = None) -> str:
    """åŸ·è¡Œæ¨ç†ï¼ˆæ”¯æ´ç´”èŠå¤©æ™‚ç”¨é«˜ temp é‡æ–°ç”Ÿæˆï¼‰
    
    v7 æ›´æ–°ï¼šdevice_list è®Šæˆå¯é¸åƒæ•¸
    - å¦‚æœæä¾› device_listï¼ŒæœƒåŒ…å«åœ¨ prompt ä¸­ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
    - å¦‚æœä¸æä¾›ï¼Œä½¿ç”¨ç´”ç²¹çš„ user requestï¼ˆæ–°æ¶æ§‹ï¼‰
    
    Args:
        user_input: ç”¨æˆ¶è¼¸å…¥
        device_list: è¨­å‚™åˆ—è¡¨ï¼ˆå¯é¸ï¼‰
        history: å°è©±æ­·å² [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]
    """
    print(f"ğŸ¯ é–‹å§‹æ¨ç† - ä½¿ç”¨è€…è¼¸å…¥: {user_input}")
    logger.info(f"ğŸ¯ é–‹å§‹æ¨ç† - ä½¿ç”¨è€…è¼¸å…¥: {user_input}")
    
    if history:
        logger.info(f"ğŸ“ æ”¶åˆ° {len(history)} æ¢æ­·å²è¨Šæ¯")
    
    # æª¢æŸ¥æ˜¯å¦åŒ…å«æ§åˆ¶é—œéµå­—æˆ–ç‹€æ…‹è©¢å•é—œéµå­—
    has_control_keyword = bool(CONTROL_KEYWORDS_PATTERN.search(user_input))
    
    # v7: æ–°å¢ç‹€æ…‹è©¢å•é—œéµå­—
    STATE_QUERY_PATTERN = re.compile(r'(é–‹è‘—å—|é—œè‘—å—|äº®è‘—å—|æ˜¯ä»€éº¼ç‹€æ…‹|ç¾åœ¨å¹¾åº¦|æº«åº¦å¤šå°‘|æœ‰é–‹å—|æœ‰é–å—|é–å¥½äº†å—)')
    has_state_query = bool(STATE_QUERY_PATTERN.search(user_input))
    
    if has_control_keyword or has_state_query:
        logger.info("ğŸ”§ æª¢æ¸¬åˆ°æ§åˆ¶/ç‹€æ…‹æŸ¥è©¢é—œéµå­—ï¼Œä½¿ç”¨ä½ temperature (0.3)")
        print("ğŸ”§ æª¢æ¸¬åˆ°æ§åˆ¶/ç‹€æ…‹æŸ¥è©¢é—œéµå­—ï¼Œä½¿ç”¨ä½ temperature")
        initial_temp = 0.3
    else:
        logger.info("ğŸ’¬ æœªæª¢æ¸¬åˆ°æ§åˆ¶é—œéµå­—ï¼Œå…ˆç”¨ä¸­ä½ temperature (0.3) å˜—è©¦")
        print("ğŸ’¬ æœªæª¢æ¸¬åˆ°æ§åˆ¶é—œéµå­—ï¼Œå…ˆç”¨ä¸­ä½ temperature å˜—è©¦")
        initial_temp = 0.3
    
    # v7: æ ¹æ“šæ˜¯å¦æœ‰ device_list æ±ºå®š prompt æ ¼å¼
    if device_list:
        user_message = f"Available devices:\n{device_list}\n\nUser request: {user_input}"
        logger.debug(f"ğŸ“‹ è¨­å‚™åˆ—è¡¨:\n{device_list}")
    else:
        user_message = f"User request:\n{user_input}"
        logger.info("ğŸ“‹ v7 æ¨¡å¼ï¼šä¸ä½¿ç”¨è¨­å‚™åˆ—è¡¨")
    
    current_dt = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # æ§‹å»º messagesï¼ˆåŒ…å«æ­·å²å°è©±ï¼‰
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "system", "content": f"ç¾åœ¨æ™‚é–“: {current_dt}"},
    ]
    
    # â­ æ·»åŠ å°è©±æ­·å²
    if history:
        for msg in history:
            messages.append({"role": msg["role"], "content": msg["content"]})
        logger.info(f"ğŸ“ å·²æ·»åŠ  {len(history)} æ¢æ­·å²åˆ° prompt")
    
    # æ·»åŠ ç•¶å‰ç”¨æˆ¶è¨Šæ¯
    messages.append({"role": "user", "content": user_message})
    
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    logger.debug(f"ğŸ“ Prompt é•·åº¦: {len(prompt)} å­—å…ƒ")
    
    inputs = tokenizer(
        prompt, 
        return_tensors="pt",
        max_length=MAX_SEQ_LENGTH,
        truncation=True
    ).to("cuda")
    
    print(f"ğŸ”„ æ¨¡å‹ç”Ÿæˆä¸­ (temperature={initial_temp})...")
    logger.info(f"ğŸ”„ æ¨¡å‹ç”Ÿæˆä¸­ (temperature={initial_temp})...")
    outputs = model.generate(
        **inputs,
        max_new_tokens=256,
        temperature=initial_temp,
        top_p=0.85,
        do_sample=True,
        repetition_penalty=1.1,
        pad_token_id=tokenizer.eos_token_id
    )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # æå– assistant çš„å›æ‡‰
    if "<|im_start|>assistant" in response:
        response = response.split("<|im_start|>assistant")[-1].strip()
    elif "assistant" in response:
        response = response.split("assistant")[-1].strip()
    
    print(f"âœ… åˆæ­¥æ¨ç†å®Œæˆ - å›æ‡‰: {response[:100]}{'...' if len(response) > 100 else ''}")
    logger.info(f"âœ… åˆæ­¥æ¨ç†å®Œæˆ - å›æ‡‰: {response[:100]}{'...' if len(response) > 100 else ''}")
    
    # â­ æª¢æ¸¬å›é¡§æ€§/ä¸Šä¸‹æ–‡ç›¸é—œå•é¡Œï¼ˆé€™äº›å•é¡Œçš„ç¬¬ä¸€æ¬¡å›ç­”é€šå¸¸æ˜¯æ­£ç¢ºçš„ï¼Œä¸æ‡‰é‡æ–°ç”Ÿæˆï¼‰
    CONTEXT_QUESTION_PATTERN = re.compile(r'(å‰›æ‰|å‰›å‰›|ä¹‹å‰|ä¸Šä¸€å€‹|ä¸Šæ¬¡|å‰›èªª|å‰›åš|èªªäº†ä»€éº¼|åšäº†ä»€éº¼|ä»€éº¼æŒ‡ä»¤|ä»€éº¼å‘½ä»¤)')
    is_context_question = bool(CONTEXT_QUESTION_PATTERN.search(user_input))
    
    # å¦‚æœæ²’æœ‰æ§åˆ¶/ç‹€æ…‹æŸ¥è©¢é—œéµå­— ä¸” æ²’æœ‰ ACTION ä¸” ä¸æ˜¯å›é¡§æ€§å•é¡Œï¼Œç”¨é«˜ temp é‡æ–°ç”Ÿæˆ
    if not has_control_keyword and not has_state_query and "ACTION" not in response and not is_context_question:
        logger.info("ğŸ”„ æª¢æ¸¬åˆ°ç´”èŠå¤©ï¼ˆç„¡ ACTIONï¼‰ï¼Œä½¿ç”¨é«˜éš¨æ©Ÿæ€§åƒæ•¸é‡æ–°ç”Ÿæˆ")
        print("ğŸ”„ æª¢æ¸¬åˆ°ç´”èŠå¤©ï¼Œä½¿ç”¨é«˜éš¨æ©Ÿæ€§åƒæ•¸é‡æ–°ç”Ÿæˆä»¥å¢åŠ è®ŠåŒ–æ€§")
        
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.85,  # æé«˜ temperature å¢åŠ éš¨æ©Ÿæ€§
            top_p=0.9,  # æé«˜ top_p è€ƒæ…®æ›´å¤šé¸é …
            top_k=50,  # åŠ å…¥ top_k é™åˆ¶åœ¨å‰ 50 å€‹ token ä¸­é¸æ“‡
            do_sample=True,
            repetition_penalty=1.15,  # æé«˜ repetition_penalty é¿å…é‡è¤‡
            pad_token_id=tokenizer.eos_token_id
        )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # å†æ¬¡æå– assistant çš„å›æ‡‰
        if "<|im_start|>assistant" in response:
            response = response.split("<|im_start|>assistant")[-1].strip()
        elif "assistant" in response:
            response = response.split("assistant")[-1].strip()
        
        # å¦‚æœé«˜éš¨æ©Ÿæ€§å°è‡´å‡ºç¾ ACTIONï¼ˆä»»ä½•æ ¼å¼ï¼‰ï¼Œåªä¿ç•™ç¬¬ä¸€è¡Œæ–‡å­—
        action_patterns = ["ACTION", "Action", "action:", "Action:", "Action to be taken"]
        has_unwanted_action = any(pattern in response for pattern in action_patterns)
        
        if has_unwanted_action:
            logger.warning("âš ï¸  é«˜éš¨æ©Ÿæ€§ç”Ÿæˆå‡ºç¾æ„å¤– ACTIONï¼Œåƒ…ä¿ç•™ç¬¬ä¸€è¡Œæ–‡å­—")
            print("âš ï¸  é«˜éš¨æ©Ÿæ€§ç”Ÿæˆå‡ºç¾æ„å¤– ACTIONï¼Œåƒ…ä¿ç•™ç¬¬ä¸€è¡Œæ–‡å­—")
            # åªä¿ç•™ç¬¬ä¸€è¡Œï¼ˆç´”èŠå¤©å›æ‡‰ï¼‰
            response = response.split('\n')[0].strip()
        
        # ç°¡è½‰ç¹ï¼ˆé«˜éš¨æ©Ÿæ€§å¯èƒ½å°è‡´ç°¡é«”è¼¸å‡ºï¼‰
        response = s2t_converter.convert(response)
        
        # ä¿®æ­£ä¸ç•¶ç”¨è©ï¼ˆé«˜éš¨æ©Ÿæ€§å¯èƒ½å°è‡´å¥‡æ€ªçš„è©å½™ï¼‰
        for pattern, replacement in CHAT_WORD_FIXES.items():
            if re.search(pattern, response):
                logger.info(f"ğŸ”§ ä¿®æ­£ç”¨è©: '{pattern}' â†’ '{replacement}'")
                print(f"ğŸ”§ ä¿®æ­£ç”¨è©: '{pattern}' â†’ '{replacement}'")
                response = re.sub(pattern, replacement, response)
        
        print(f"âœ¨ é«˜ temp é‡æ–°ç”Ÿæˆå®Œæˆ - å›æ‡‰: {response[:100]}{'...' if len(response) > 100 else ''}")
        logger.info(f"âœ¨ é«˜ temp é‡æ–°ç”Ÿæˆå®Œæˆ - å›æ‡‰: {response[:100]}{'...' if len(response) > 100 else ''}")
    
    return response


def run_state_result_inference(original_question: str, state_result: str, device_name: str, area: str) -> str:
    """
    v7: åŸ·è¡Œ get_state äºŒæ¬¡å°è©±æ¨ç†
    
    ç•¶æ¨¡å‹è¼¸å‡º ACTION get_state å¾Œï¼Œç³»çµ±æŸ¥è©¢è¨­å‚™ç‹€æ…‹ä¸¦è¿”å›çµæœã€‚
    æ­¤å‡½æ•¸è² è²¬æ ¹æ“šç‹€æ…‹çµæœç”Ÿæˆå›ç­”ã€‚
    
    Args:
        original_question: ç”¨æˆ¶çš„åŸå§‹å•é¡Œ
        state_result: è¨­å‚™ç‹€æ…‹çµæœ
        device_name: è¨­å‚™åç¨±
        area: å€åŸŸåç¨±
    """
    print(f"ğŸ” äºŒæ¬¡å°è©±æ¨ç† - å•é¡Œ: {original_question}")
    print(f"ğŸ“Š ç‹€æ…‹çµæœ: {device_name}@{area} = {state_result}")
    logger.info(f"ğŸ” äºŒæ¬¡å°è©±æ¨ç† - å•é¡Œ: {original_question}")
    logger.info(f"ğŸ“Š ç‹€æ…‹çµæœ: {device_name}@{area} = {state_result}")
    
    full_name = f"{area}{device_name}"
    
    # æ§‹å»ºå¤šè¼ªå°è©± prompt
    current_dt = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "system", "content": f"ç¾åœ¨æ™‚é–“: {current_dt}"},
        {"role": "user", "content": f"User request:\n{original_question}"},
        {"role": "assistant", "content": f"æˆ‘ä¾†å¹«ä½ çœ‹ä¸€ä¸‹å–µï½\nACTION get_state\nname {device_name}\narea {area}"},
        {"role": "user", "content": f"State result:\n{full_name}: {state_result}"},
    ]
    
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer(
        prompt, 
        return_tensors="pt",
        max_length=MAX_SEQ_LENGTH,
        truncation=True
    ).to("cuda")
    
    print("ğŸ”„ æ¨¡å‹ç”Ÿæˆä¸­ï¼ˆäºŒæ¬¡å°è©±æ¨¡å¼ï¼‰...")
    logger.info("ğŸ”„ æ¨¡å‹ç”Ÿæˆä¸­ï¼ˆäºŒæ¬¡å°è©±æ¨¡å¼ï¼‰...")
    outputs = model.generate(
        **inputs,
        max_new_tokens=150,
        temperature=0.3,  # ä½æº«åº¦ç¢ºä¿æº–ç¢ºæ€§
        top_p=0.85,
        do_sample=True,
        repetition_penalty=1.1,
        pad_token_id=tokenizer.eos_token_id
    )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=False)
    
    # æå– assistant çš„æœ€å¾Œä¸€å€‹å›æ‡‰
    # æ ¼å¼æ‡‰è©²æ˜¯: ...<|im_start|>assistant\n{å›æ‡‰}<|im_end|>
    if "<|im_start|>assistant" in response:
        # å–æœ€å¾Œä¸€å€‹ assistant å›æ‡‰
        parts = response.split("<|im_start|>assistant")
        last_part = parts[-1].strip()
        
        # ç§»é™¤é–‹é ­çš„æ›è¡Œ
        if last_part.startswith("\n"):
            last_part = last_part[1:]
        
        # ç§»é™¤ <|im_end|> å’Œä¹‹å¾Œçš„å…§å®¹
        if "<|im_end|>" in last_part:
            last_part = last_part.split("<|im_end|>")[0]
        
        response = last_part.strip()
    else:
        # å‚™ç”¨æ–¹æ¡ˆï¼šå˜—è©¦ç”¨ "assistant" åˆ†å‰²
        if "assistant\n" in response:
            parts = response.split("assistant\n")
            response = parts[-1].strip()
    
    # ç§»é™¤å¯èƒ½æ®˜ç•™çš„ç‰¹æ®Šç¬¦è™Ÿ
    response = response.replace("<|im_end|>", "").strip()
    response = response.replace("<|im_start|>", "").strip()
    
    print(f"âœ… äºŒæ¬¡å°è©±æ¨ç†å®Œæˆ - å›æ‡‰: {response}")
    logger.info(f"âœ… äºŒæ¬¡å°è©±æ¨ç†å®Œæˆ - å›æ‡‰: {response}")
    
    return response


def run_search_result_inference(user_question: str, search_result: str) -> str:
    """
    åŸ·è¡Œæœå°‹çµæœå›æ‡‰æ¨ç†ï¼ˆå°ˆç”¨å‡½æ•¸ï¼‰
    
    ç‰¹é»ï¼š
    - ä½¿ç”¨ç°¡çŸ­çš„ SEARCH_RESULT_PROMPT
    - ä¸éœ€è¦è£ç½®åˆ—è¡¨
    - åªéœ€è¦ï¼šåŸå§‹å•é¡Œ + æœå°‹çµæœ
    - ç”Ÿæˆç°¡çŸ­å¯æ„›çš„å›æ‡‰
    """
    print(f"ğŸ” æœå°‹çµæœæ¨ç† - å•é¡Œ: {user_question}")
    print(f"ğŸ“Š æœå°‹çµæœ: {search_result[:100]}{'...' if len(search_result) > 100 else ''}")
    logger.info(f"ğŸ” æœå°‹çµæœæ¨ç† - å•é¡Œ: {user_question}")
    logger.info(f"ğŸ“Š æœå°‹çµæœ: {search_result[:100]}{'...' if len(search_result) > 100 else ''}")
    
    # æ¸…ç†è¼¸å…¥ï¼ˆå»é™¤å¤šé¤˜ç©ºæ ¼å’Œæ›è¡Œï¼‰
    user_question_clean = ' '.join(user_question.strip().split())
    search_result_clean = ' '.join(search_result.strip().split())
    
    # æ§‹å»º user prompt - å¼·èª¿ä¿ç•™æ‰€æœ‰æ•¸æ“šä¸¦ä¿æŒç°¡æ½”
    user_message = f'ä½¿ç”¨è€…å•ï¼š"{user_question_clean}"ï¼Œæœå°‹çµæœï¼š"{search_result_clean}"ã€‚è«‹ç”¨è²“å¨˜é¢¨æ ¼ç°¡æ½”åœ°è½‰è¿°é€™äº›è³‡è¨Šï¼Œã€çµ•å°ä¸èƒ½æ”¹è®Šä»»ä½•æ•¸å­—ã€‘ï¼Œã€ä¸è¦é¡å¤–è§£ææˆ–ç¸½çµã€‘ï¼Œç›´æ¥ç”¨å¯æ„›çš„èªæ°£èªªå‡ºä¾†å°±å¥½å–µï¼'
    
    current_dt = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    messages = [
        {"role": "system", "content": SEARCH_RESULT_PROMPT},
        {"role": "system", "content": f"ç¾åœ¨æ™‚é–“: {current_dt}"},
        {"role": "user", "content": user_message}
    ]
    
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer(
        prompt, 
        return_tensors="pt",
        max_length=MAX_SEQ_LENGTH,
        truncation=True
    ).to("cuda")
    
    print("ğŸ”„ æ¨¡å‹ç”Ÿæˆä¸­ï¼ˆæœå°‹çµæœæ¨¡å¼ï¼‰...")
    logger.info("ğŸ”„ æ¨¡å‹ç”Ÿæˆä¸­ï¼ˆæœå°‹çµæœæ¨¡å¼ï¼‰...")
    outputs = model.generate(
        **inputs,
        max_new_tokens=150,  # é©ä¸­é•·åº¦ï¼Œé¿å…éåº¦è§£é‡‹
        temperature=0.3,  # é™ä½æº«åº¦ä»¥ä¿æŒè³‡è¨Šæº–ç¢ºæ€§
        top_p=0.85,
        do_sample=True,
        repetition_penalty=1.1,
        pad_token_id=tokenizer.eos_token_id
    )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # æå– assistant çš„å›æ‡‰
    if "<|im_start|>assistant" in response:
        response = response.split("<|im_start|>assistant")[-1].strip()
    elif "assistant" in response:
        response = response.split("assistant")[-1].strip()
    
    # ç§»é™¤å°¾éƒ¨ç‰¹æ®Šç¬¦è™Ÿ
    response = response.rstrip()
    if response.endswith("<|im_end|>"):
        response = response[:-10].rstrip()
    
    # å®Œæ•´è¼¸å‡ºå›æ‡‰ï¼ˆä¸æˆªæ–·ï¼‰
    print(f"âœ… æœå°‹çµæœæ¨ç†å®Œæˆ - å›æ‡‰:")
    print(response)
    logger.info(f"âœ… æœå°‹çµæœæ¨ç†å®Œæˆ - å›æ‡‰:")
    logger.info(response)
    
    return response


def run_fallback_assistant_inference(user_question: str, assistant_response: str) -> str:
    """
    åŸ·è¡Œ Fallback åŠ©ç†å›æ‡‰å–µåŒ–æ¨ç†
    
    ä½¿ç”¨å ´æ™¯ï¼š
    - ç•¶æŒ‡ä»¤ fallback åˆ°å…¶ä»–åŠ©ç†ï¼ˆGoogle Assistantã€Alexa ç­‰ï¼‰
    - æ”¶åˆ°å…¶ä»–åŠ©ç†çš„å›æ‡‰å¾Œ
    - å°‡å›æ‡‰å–µåŒ–å¾Œå†é€å‡ºçµ¦ä½¿ç”¨è€…
    
    ç‰¹é»ï¼š
    - ä½¿ç”¨å°ˆç”¨çš„ FALLBACK_ASSISTANT_PROMPT
    - ä¿ç•™åŸå§‹å›æ‡‰çš„å®Œæ•´è³‡è¨Š
    - åŠ å…¥è²“å¨˜é¢¨æ ¼å’Œã€Œå–µã€
    """
    print(f"ğŸ­ Fallback åŠ©ç†å–µåŒ– - å•é¡Œ: {user_question}")
    print(f"ğŸ’¬ åŠ©ç†å›æ‡‰: {assistant_response}")
    logger.info(f"ğŸ­ Fallback åŠ©ç†å–µåŒ– - å•é¡Œ: {user_question}")
    logger.info(f"ğŸ’¬ åŠ©ç†å›æ‡‰: {assistant_response}")
    
    # æ¸…ç†è¼¸å…¥
    user_question_clean = ' '.join(user_question.strip().split())
    assistant_response_clean = ' '.join(assistant_response.strip().split())
    
    # æ§‹å»º user prompt - å¼·èª¿ä¸èƒ½æ”¹è®Šæ•¸å­—
    user_message = f'ä½¿ç”¨è€…å•ï¼š"{user_question_clean}"ï¼Œå…¶ä»–åŠ©ç†å›ç­”ï¼š"{assistant_response_clean}"ã€‚è«‹ç”¨è²“å¨˜é¢¨æ ¼é‡æ–°è¡¨é”ï¼Œä½†ã€çµ•å°ä¸èƒ½æ”¹è®Šä»»ä½•æ•¸å­—ã€æº«åº¦ã€æ™‚é–“ç­‰è³‡è¨Šã€‘ï¼Œåªæ”¹è®Šèªæ°£åŠ å…¥ã€Œå–µã€ï¼'
    
    messages = [
        {"role": "system", "content": FALLBACK_ASSISTANT_PROMPT},
        {"role": "user", "content": user_message}
    ]
    
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer(
        prompt, 
        return_tensors="pt",
        max_length=MAX_SEQ_LENGTH,
        truncation=True
    ).to("cuda")
    
    print("ğŸ”„ æ¨¡å‹ç”Ÿæˆä¸­ï¼ˆFallback åŠ©ç†å–µåŒ–æ¨¡å¼ï¼‰...")
    logger.info("ğŸ”„ æ¨¡å‹ç”Ÿæˆä¸­ï¼ˆFallback åŠ©ç†å–µåŒ–æ¨¡å¼ï¼‰...")
    outputs = model.generate(
        **inputs,
        max_new_tokens=150,  # ç¨é•·ä¸€äº›ä»¥ä¿ç•™å®Œæ•´è³‡è¨Š
        temperature=0.3,  # é™ä½æº«åº¦ä»¥ä¿æŒè³‡è¨Šæº–ç¢ºæ€§ï¼ˆå¾ 0.6 æ”¹ç‚º 0.3ï¼‰
        top_p=0.85,
        do_sample=True,
        repetition_penalty=1.1,
        pad_token_id=tokenizer.eos_token_id
    )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # æå– assistant çš„å›æ‡‰
    if "<|im_start|>assistant" in response:
        response = response.split("<|im_start|>assistant")[-1].strip()
    elif "assistant" in response:
        response = response.split("assistant")[-1].strip()
    
    # ç§»é™¤å°¾éƒ¨ç‰¹æ®Šç¬¦è™Ÿ
    response = response.rstrip()
    if response.endswith("<|im_end|>"):
        response = response[:-10].rstrip()
    
    # å®Œæ•´è¼¸å‡ºå›æ‡‰ï¼ˆä¸æˆªæ–·ï¼‰
    print(f"âœ… Fallback åŠ©ç†å–µåŒ–å®Œæˆ - å›æ‡‰:")
    print(response)
    logger.info(f"âœ… Fallback åŠ©ç†å–µåŒ–å®Œæˆ - å›æ‡‰:")
    logger.info(response)
    
    return response

# ==============================================================================
# API Endpoints
# ==============================================================================
@app.get("/")
async def root():
    return {
        "status": "ok", 
        "message": "Qwen Catgirl Home Assistant API v7",
        "version": "v7 (get_state architecture)",
        "model": MODEL_PATH
    }

@app.get("/health")
async def health():
    return {"status": "healthy", "model_loaded": model is not None}

@app.post("/process", response_model=ActionResult)
async def process_request(request: InferenceRequest):
    """
    è™•ç†ç”¨æˆ¶è«‹æ±‚ (v7 ç‰ˆæœ¬ - ä¸å¼·åˆ¶è¦æ±‚ devices)
    
    v7 è®Šæ›´ï¼š
    - devices è®Šæˆå¯é¸åƒæ•¸
    - å¦‚æœä¸æä¾› devicesï¼Œæ¨¡å‹å°‡ä¸çŸ¥é“è¨­å‚™ç‹€æ…‹ï¼Œæœƒä¸»å‹•èª¿ç”¨ get_state æŸ¥è©¢
    """
    print("=" * 60)
    print(f"ğŸ“¨ æ”¶åˆ° /process è«‹æ±‚")
    print(f"ğŸ’¬ ä½¿ç”¨è€…è¼¸å…¥: {request.text}")
    
    # v7: devices æ˜¯å¯é¸çš„
    device_count = len(request.devices) if request.devices else 0
    print(f"ğŸ”§ è¨­å‚™æ•¸é‡: {device_count} {'(v7: ä¸ä½¿ç”¨è¨­å‚™åˆ—è¡¨)' if device_count == 0 else ''}")
    
    logger.info("=" * 60)
    logger.info(f"ğŸ“¨ æ”¶åˆ° /process è«‹æ±‚")
    logger.info(f"ğŸ’¬ ä½¿ç”¨è€…è¼¸å…¥: {request.text}")
    logger.info(f"ğŸ”§ è¨­å‚™æ•¸é‡: {device_count}")
    
    if model is None:
        logger.error("âŒ æ¨¡å‹æœªè¼‰å…¥")
        print("âŒ æ¨¡å‹æœªè¼‰å…¥")
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    try:
        # v7: æ ¼å¼åŒ–è¨­å‚™åˆ—è¡¨ï¼ˆå¦‚æœæœ‰æä¾›ï¼‰
        device_list = None
        if request.devices:
            device_list = devices_to_domain_grouped_format(request.devices)
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºæ™‚é–“æˆ–æ—¥æœŸè©¢å•ï¼Œè‹¥æ˜¯ç›´æ¥ç”¨æ¨¡æ¿å›è¦†å†äº¤ç”± fallback åŠ©ç†é€²è¡Œå–µåŒ–è™•ç†
        if TIME_QUESTION_PATTERN.search(request.text):
            now = datetime.now()
            hour = now.hour
            minute = now.minute
            period = 'ä¸Šåˆ' if hour < 12 else 'ä¸‹åˆ'
            hour12 = hour if 1 <= hour <= 12 else (hour - 12 if hour > 12 else 12)
            assistant_response = f"ç¾åœ¨æ˜¯{period}{hour12}:{minute:02d}"
            logger.info(f"â° åµæ¸¬åˆ°æ™‚é–“è©¢å•ï¼Œå›å‚³æ¨¡æ¿: {assistant_response}")
            print(f"â° åµæ¸¬åˆ°æ™‚é–“è©¢å•ï¼Œå›å‚³æ¨¡æ¿: {assistant_response}")

            # ä½¿ç”¨ fallback åŠ©ç†é€²è¡Œå–µåŒ–è™•ç†
            response = run_fallback_assistant_inference(request.text, assistant_response)
            action_data = None
        elif DATE_QUESTION_PATTERN.search(request.text):
            today = datetime.now()
            # ç›´æ¥ä½¿ç”¨å–µåŒ–æ¨¡æ¿å›è¦†ï¼Œä¸ç¶“ç”± LLM è™•ç†
            assistant_response = f"ä»Šå¤©æ˜¯{today.year}å¹´{today.month}æœˆ{today.day}æ—¥å–µï½"
            logger.info(f"ğŸ“… åµæ¸¬åˆ°æ—¥æœŸè©¢å•ï¼Œå›å‚³æ¨¡æ¿ï¼ˆå·²å–µåŒ–ï¼‰: {assistant_response}")
            print(f"ğŸ“… åµæ¸¬åˆ°æ—¥æœŸè©¢å•ï¼Œå›å‚³æ¨¡æ¿ï¼ˆå·²å–µåŒ–ï¼‰: {assistant_response}")

            # ä¸å‘¼å« LLMï¼Œç›´æ¥å›å‚³å·²å–µåŒ–çš„æ–‡å­—
            response = assistant_response
            action_data = None
        else:
            # åŸ·è¡Œæ¨ç†ï¼ˆä¸€èˆ¬æµç¨‹ï¼ŒåŒ…å«å°è©±æ­·å²ï¼‰
            response = run_inference(request.text, device_list, request.history)
            
            # è§£æ ACTION (v6 æ ¼å¼)
            action_data = parse_action_from_response(response)
        
        if action_data:
            logger.info(f"âš¡ è§£æåˆ° ACTION: {action_data['action']}")
            logger.info(f"ğŸ“‹ åƒæ•¸: {action_data['params']}")
            print(f"âš¡ è§£æåˆ° ACTION: {action_data['action']}")
            print(f"ğŸ“‹ åƒæ•¸: {action_data['params']}")
        else:
            logger.info("ğŸ’­ ç„¡ ACTIONï¼ˆç´”èŠå¤©ï¼‰")
            print("ğŸ’­ ç„¡ ACTIONï¼ˆç´”èŠå¤©ï¼‰")
        
        # æå–ç´”æ–‡å­—å›æ‡‰ï¼ˆå»é™¤ ACTION éƒ¨åˆ†ï¼‰
        response_lines = response.split('\n')
        response_text = []
        for line in response_lines:
            if not line.strip().startswith(('ACTION', 'name', 'area', 'brightness', 'color', 'temperature', 'command', 'position', 'volume', 'mode')):
                response_text.append(line)
        response_text = '\n'.join(response_text).strip()
        
        logger.info(f"ğŸ“¤ è¿”å›å›æ‡‰: {response_text}")
        logger.info("=" * 60)
        print(f"ğŸ“¤ è¿”å›å›æ‡‰: {response_text}")
        print("=" * 60)
        
        return ActionResult(
            action=action_data['action'] if action_data else None,
            params=action_data['params'] if action_data else {},
            response_text=response_text,
            has_action=action_data is not None,
            raw_response=response,  # ä¿å­˜å®Œæ•´å›æ‡‰ï¼ˆåŒ…å« ACTIONï¼‰
        )
        
    except Exception as e:
        import traceback
        logger.error("âŒ è™•ç†è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤:")
        logger.error(traceback.format_exc())
        print("âŒ è™•ç†è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤:")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/process_with_state", response_model=SearchResultResponse)
async def process_with_state(request: StateResultRequest):
    """
    v7: è™•ç† get_state äºŒæ¬¡å°è©±
    
    ä½¿ç”¨å ´æ™¯ï¼š
    1. ç”¨æˆ¶è©¢å•è¨­å‚™ç‹€æ…‹ï¼ˆä¾‹å¦‚ï¼šã€Œå®¢å»³ç‡ˆé–‹è‘—å—ã€ï¼‰
    2. æ¨¡å‹è¼¸å‡º ACTION get_state
    3. Home Assistant æŸ¥è©¢è¨­å‚™ç‹€æ…‹
    4. å‘¼å«æ­¤ APIï¼Œå‚³å…¥åŸå§‹å•é¡Œå’Œç‹€æ…‹çµæœ
    5. æ¨¡å‹æ ¹æ“šç‹€æ…‹çµæœç”Ÿæˆå›ç­”
    
    åƒæ•¸ï¼š
    - original_question: ç”¨æˆ¶çš„åŸå§‹å•é¡Œ
    - state_result: è¨­å‚™ç‹€æ…‹ï¼ˆä¾‹å¦‚ï¼šã€Œonã€ã€Œoffã€ã€Œcool, 26Â°Cã€ï¼‰
    - device_name: è¨­å‚™åç¨±
    - area: å€åŸŸåç¨±
    """
    print("=" * 60)
    print("ğŸ“¨ æ”¶åˆ° /process_with_state è«‹æ±‚")
    print(f"â“ åŸå§‹å•é¡Œ: {request.original_question}")
    print(f"ğŸ“Š ç‹€æ…‹çµæœ: {request.device_name}@{request.area} = {request.state_result}")
    
    logger.info("=" * 60)
    logger.info("ğŸ“¨ æ”¶åˆ° /process_with_state è«‹æ±‚")
    logger.info(f"â“ åŸå§‹å•é¡Œ: {request.original_question}")
    logger.info(f"ğŸ“Š ç‹€æ…‹çµæœ: {request.device_name}@{request.area} = {request.state_result}")
    
    if model is None:
        logger.error("âŒ æ¨¡å‹æœªè¼‰å…¥")
        print("âŒ æ¨¡å‹æœªè¼‰å…¥")
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    try:
        # åŸ·è¡ŒäºŒæ¬¡å°è©±æ¨ç†
        response_text = run_state_result_inference(
            original_question=request.original_question,
            state_result=request.state_result,
            device_name=request.device_name,
            area=request.area
        )
        
        print("=" * 60)
        logger.info("=" * 60)
        
        return SearchResultResponse(
            response_text=response_text
        )
        
    except Exception as e:
        import traceback
        logger.error("âŒ è™•ç† get_state äºŒæ¬¡å°è©±æ™‚ç™¼ç”ŸéŒ¯èª¤:")
        logger.error(traceback.format_exc())
        print("âŒ è™•ç† get_state äºŒæ¬¡å°è©±æ™‚ç™¼ç”ŸéŒ¯èª¤:")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/search_result", response_model=SearchResultResponse)
async def process_search_result(request: SearchResultRequest):
    """
    è™•ç†æœå°‹çµæœå›æ‡‰ï¼ˆä¸éœ€è¦è£ç½®åˆ—è¡¨ï¼‰
    
    ä½¿ç”¨å ´æ™¯ï¼š
    1. ä½¿ç”¨è€…æå‡ºéœ€è¦æœå°‹çš„å•é¡Œï¼ˆä¾‹å¦‚ï¼šã€Œé€±äº”ä¸­å’Œå€æœƒä¸‹é›¨å—ã€ï¼‰
    2. ç³»çµ±ä½¿ç”¨æœå°‹å·¥å…·ç²å¾—çµæœ
    3. å‘¼å«æ­¤ APIï¼Œå‚³å…¥åŸå§‹å•é¡Œå’Œæœå°‹çµæœ
    4. æ¨¡å‹ç”Ÿæˆç°¡çŸ­ã€å¯æ„›çš„å›æ‡‰
    
    ç‰¹é»ï¼š
    - ä½¿ç”¨ç°¡çŸ­çš„ SEARCH_RESULT_PROMPTï¼ˆè€Œéå®Œæ•´çš„ SYSTEM_PROMPTï¼‰
    - ä¸éœ€è¦æä¾›è£ç½®åˆ—è¡¨
    - è‡ªå‹•æ¸…ç†æ›è¡Œå’Œå¤šé¤˜ç©ºæ ¼
    - åªè¿”å›æ–‡å­—å›æ‡‰ï¼ˆä¸åŒ…å« ACTIONï¼‰
    """
    print("=" * 60)
    print("ğŸ“¨ æ”¶åˆ° /search_result è«‹æ±‚")
    logger.info("=" * 60)
    logger.info("ğŸ“¨ æ”¶åˆ° /search_result è«‹æ±‚")
    
    if model is None:
        logger.error("âŒ æ¨¡å‹æœªè¼‰å…¥")
        print("âŒ æ¨¡å‹æœªè¼‰å…¥")
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    try:
        # åŸ·è¡Œæœå°‹çµæœæ¨ç†
        response_text = run_search_result_inference(
            user_question=request.user_question,
            search_result=request.search_result
        )
        
        print("=" * 60)
        logger.info("=" * 60)
        
        return SearchResultResponse(
            response_text=response_text
        )
        
    except Exception as e:
        import traceback
        logger.error("âŒ è™•ç†æœå°‹çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤:")
        logger.error(traceback.format_exc())
        print("âŒ è™•ç†æœå°‹çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤:")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/fallback_assistant", response_model=SearchResultResponse)
async def process_fallback_assistant(request: FallbackAssistantRequest):
    """
    è™•ç† Fallback åŠ©ç†å›æ‡‰å–µåŒ–
    
    ä½¿ç”¨å ´æ™¯ï¼š
    1. ä½¿ç”¨è€…çš„æŒ‡ä»¤å› å„ç¨®å› ç´  fallback åˆ°å…¶ä»–åŠ©ç†ï¼ˆGoogle Assistantã€Alexa ç­‰ï¼‰
    2. å…¶ä»–åŠ©ç†åŸ·è¡Œå®Œç•¢ä¸¦è¿”å›å›æ‡‰
    3. å‘¼å«æ­¤ APIï¼Œå‚³å…¥åŸå§‹å•é¡Œå’Œå…¶ä»–åŠ©ç†çš„å›æ‡‰
    4. æ¨¡å‹å°‡å›æ‡‰å–µåŒ–å¾Œè¿”å›
    
    ç¯„ä¾‹ï¼š
    - ä½¿ç”¨è€…ï¼šã€Œæé†’æˆ‘æ˜å¤©ä¸‹åˆ3é»é–‹æœƒã€
    - Google Assistantï¼šã€Œå¥½çš„ï¼Œæˆ‘å·²ç¶“ç‚ºä½ è¨­å®šæ˜å¤©ä¸‹åˆ3é»çš„æé†’ã€
    - æœ¬ API å–µåŒ–å¾Œï¼šã€Œå¥½çš„å–µï¼å·²ç¶“å¹«ä½ è¨­å®šæ˜å¤©ä¸‹åˆ3é»çš„æé†’äº†å–µï½ã€
    
    ç‰¹é»ï¼š
    - ä½¿ç”¨å°ˆç”¨çš„ FALLBACK_ASSISTANT_PROMPT
    - ä¿ç•™åŸå§‹å›æ‡‰çš„å®Œæ•´è³‡è¨Šå’Œæ„æ€
    - åŠ å…¥è²“å¨˜é¢¨æ ¼å’Œã€Œå–µã€
    - åªè¿”å›æ–‡å­—å›æ‡‰ï¼ˆä¸åŒ…å« ACTIONï¼‰
    """
    print("=" * 60)
    print("ğŸ“¨ æ”¶åˆ° /fallback_assistant è«‹æ±‚")
    logger.info("=" * 60)
    logger.info("ğŸ“¨ æ”¶åˆ° /fallback_assistant è«‹æ±‚")
    
    if model is None:
        logger.error("âŒ æ¨¡å‹æœªè¼‰å…¥")
        print("âŒ æ¨¡å‹æœªè¼‰å…¥")
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    try:
        # åŸ·è¡Œ Fallback åŠ©ç†å›æ‡‰å–µåŒ–æ¨ç†
        response_text = run_fallback_assistant_inference(
            user_question=request.user_question,
            assistant_response=request.assistant_response
        )
        
        print("=" * 60)
        logger.info("=" * 60)
        
        return SearchResultResponse(
            response_text=response_text
        )
        
    except Exception as e:
        import traceback
        logger.error("âŒ è™•ç† Fallback åŠ©ç†å–µåŒ–æ™‚ç™¼ç”ŸéŒ¯èª¤:")
        logger.error(traceback.format_exc())
        print("âŒ è™•ç† Fallback åŠ©ç†å–µåŒ–æ™‚ç™¼ç”ŸéŒ¯èª¤:")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

# ==============================================================================
# ä¸»ç¨‹åº
# ==============================================================================
if __name__ == "__main__":
    logger.info("ğŸš€ å•Ÿå‹• Qwen Catgirl Home Assistant API Server v6")
    logger.info(f"ğŸ“¡ ç›£è½: {HOST}:{PORT}")
    logger.info("ğŸ’¡ v6 ç‰¹æ€§: name + area æ ¼å¼ï¼Œé›¶ entity_id å¹»è¦º")
    uvicorn.run(app, host=HOST, port=PORT)

