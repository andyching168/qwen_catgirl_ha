#!/usr/bin/env python3
"""
BERT æ„åœ–åˆ†é¡å™¨è¨“ç·´è…³æœ¬

æ”¯æ´ç’°å¢ƒï¼š
- M1/M2 Mac (MPS åŠ é€Ÿ)
- CUDA GPU
- CPU (è¼ƒæ…¢)

æ¨¡å‹ï¼šhfl/rbt3 (38M åƒæ•¸ï¼Œè¼•é‡ç´šä¸­æ–‡ BERT)
"""

import json
import torch
import numpy as np
from pathlib import Path
from dataclasses import dataclass
from typing import Optional
import argparse

from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback,
)
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix

# ==============================================================================
# é…ç½®
# ==============================================================================

@dataclass
class Config:
    model_name: str = "hfl/rbt3"  # è¼•é‡ç´šä¸­æ–‡ BERT
    max_length: int = 64  # æ™ºæ…§å®¶å±…æŒ‡ä»¤é€šå¸¸å¾ˆçŸ­
    num_labels: int = 5  # turn_on, turn_off, climate_set_mode, get_state, chat
    batch_size: int = 32
    learning_rate: float = 2e-5
    num_epochs: int = 5
    warmup_ratio: float = 0.1
    weight_decay: float = 0.01
    
    # è·¯å¾‘
    train_file: str = "bert_training_data/intent_train.jsonl"
    test_file: str = "bert_training_data/intent_test.jsonl"
    output_dir: str = "bert_intent_model"

# æ¨™ç±¤åç¨±
LABEL_NAMES = ["turn_on", "turn_off", "climate_set_mode", "get_state", "chat"]

# ==============================================================================
# è³‡æ–™é›†
# ==============================================================================

class IntentDataset(Dataset):
    def __init__(self, file_path: str, tokenizer, max_length: int):
        self.samples = []
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data = json.loads(line)
                    self.samples.append({
                        "text": data["text"],
                        "label": data["label"]
                    })
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        encoding = self.tokenizer(
            sample["text"],
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        
        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "labels": torch.tensor(sample["label"], dtype=torch.long)
        }

# ==============================================================================
# è¨“ç·´å‡½æ•¸
# ==============================================================================

def compute_metrics(eval_pred):
    """è¨ˆç®—è©•ä¼°æŒ‡æ¨™"""
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    
    accuracy = accuracy_score(labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, predictions, average='weighted', zero_division=0
    )
    
    return {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1": f1,
    }

def get_device():
    """è‡ªå‹•æª¢æ¸¬æœ€ä½³è¨­å‚™"""
    if torch.cuda.is_available():
        device = "cuda"
        print(f"ğŸš€ ä½¿ç”¨ CUDA: {torch.cuda.get_device_name(0)}")
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        device = "mps"
        print("ğŸ ä½¿ç”¨ Apple MPS")
    else:
        device = "cpu"
        print("ğŸ’» ä½¿ç”¨ CPU")
    
    return device

def main():
    parser = argparse.ArgumentParser(description="è¨“ç·´ BERT æ„åœ–åˆ†é¡å™¨")
    parser.add_argument("--epochs", type=int, default=5, help="è¨“ç·´è¼ªæ•¸")
    parser.add_argument("--batch_size", type=int, default=32, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr", type=float, default=2e-5, help="å­¸ç¿’ç‡")
    parser.add_argument("--model", type=str, default="hfl/rbt3", help="é è¨“ç·´æ¨¡å‹")
    args = parser.parse_args()
    
    config = Config(
        model_name=args.model,
        batch_size=args.batch_size,
        learning_rate=args.lr,
        num_epochs=args.epochs,
    )
    
    print("=" * 60)
    print("BERT æ„åœ–åˆ†é¡å™¨è¨“ç·´")
    print("=" * 60)
    print(f"æ¨¡å‹: {config.model_name}")
    print(f"æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    print(f"å­¸ç¿’ç‡: {config.learning_rate}")
    print(f"è¨“ç·´è¼ªæ•¸: {config.num_epochs}")
    print("=" * 60)
    
    # æª¢æ¸¬è¨­å‚™
    device = get_device()
    
    # è¼‰å…¥ tokenizer å’Œæ¨¡å‹
    print("\nğŸ“¥ è¼‰å…¥æ¨¡å‹å’Œ tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    model = AutoModelForSequenceClassification.from_pretrained(
        config.model_name,
        num_labels=config.num_labels,
        id2label={i: name for i, name in enumerate(LABEL_NAMES)},
        label2id={name: i for i, name in enumerate(LABEL_NAMES)},
    )
    
    # è¼‰å…¥è³‡æ–™é›†
    print("\nğŸ“‚ è¼‰å…¥è³‡æ–™é›†...")
    train_dataset = IntentDataset(config.train_file, tokenizer, config.max_length)
    test_dataset = IntentDataset(config.test_file, tokenizer, config.max_length)
    
    print(f"   è¨“ç·´é›†: {len(train_dataset)} ç­†")
    print(f"   æ¸¬è©¦é›†: {len(test_dataset)} ç­†")
    
    # çµ±è¨ˆæ¨™ç±¤åˆ†å¸ƒ
    train_labels = [s["label"] for s in train_dataset.samples]
    print("\nğŸ“Š è¨“ç·´é›†æ¨™ç±¤åˆ†å¸ƒ:")
    for label_id, label_name in enumerate(LABEL_NAMES):
        count = train_labels.count(label_id)
        print(f"   {label_name}: {count}")
    
    # è¨“ç·´åƒæ•¸
    training_args = TrainingArguments(
        output_dir=config.output_dir,
        num_train_epochs=config.num_epochs,
        per_device_train_batch_size=config.batch_size,
        per_device_eval_batch_size=config.batch_size,
        warmup_ratio=config.warmup_ratio,
        weight_decay=config.weight_decay,
        learning_rate=config.learning_rate,
        logging_dir=f"{config.output_dir}/logs",
        logging_steps=50,
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        greater_is_better=True,
        report_to="none",  # ä¸ä½¿ç”¨ wandb ç­‰
        # MPS ç‰¹å®šè¨­å®š
        use_mps_device=(device == "mps"),
        dataloader_num_workers=0 if device == "mps" else 4,
    )
    
    # å»ºç«‹ Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],
    )
    
    # é–‹å§‹è¨“ç·´
    print("\nğŸ‹ï¸ é–‹å§‹è¨“ç·´...")
    trainer.train()
    
    # è©•ä¼°
    print("\nğŸ“ˆ è©•ä¼°çµæœ:")
    results = trainer.evaluate()
    for key, value in results.items():
        if isinstance(value, float):
            print(f"   {key}: {value:.4f}")
    
    # å„²å­˜æ¨¡å‹
    print(f"\nğŸ’¾ å„²å­˜æ¨¡å‹åˆ°: {config.output_dir}")
    trainer.save_model(config.output_dir)
    tokenizer.save_pretrained(config.output_dir)
    
    # é¡¯ç¤ºæ··æ·†çŸ©é™£
    print("\nğŸ”¢ æ··æ·†çŸ©é™£:")
    predictions = trainer.predict(test_dataset)
    pred_labels = np.argmax(predictions.predictions, axis=-1)
    true_labels = predictions.label_ids
    
    cm = confusion_matrix(true_labels, pred_labels)
    print("       " + "  ".join([f"{name[:6]:>6}" for name in LABEL_NAMES]))
    for i, row in enumerate(cm):
        print(f"{LABEL_NAMES[i][:6]:>6} " + "  ".join([f"{val:>6}" for val in row]))
    
    print("\nâœ… è¨“ç·´å®Œæˆï¼")
    print(f"   æ¨¡å‹ä½ç½®: {config.output_dir}")
    print(f"   æ¨™ç±¤æ•¸é‡: {config.num_labels}")
    print(f"   æ¨™ç±¤åˆ—è¡¨: {LABEL_NAMES}")

if __name__ == "__main__":
    main()
