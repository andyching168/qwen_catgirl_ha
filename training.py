#!/usr/bin/env python3
"""
Qwen è²“å¨˜è¨“ç·´è…³æœ¬ï¼ˆæœ€ç°¡åŒ–ç‰ˆæœ¬ï¼‰
"""

from unsloth import FastLanguageModel
from datasets import load_dataset
from trl import SFTTrainer
from transformers import TrainingArguments

MODEL_NAME = "unsloth/Qwen2.5-7B-Instruct"
TRAIN_FILE = "training_data_v7_get_state.jsonl"
OUTPUT_DIR = "./qwen-catgirl-ha-switch-v2"
MAX_SEQ_LENGTH = 1280

print("ğŸ¦¥ Unsloth è¨“ç·´")
print("=" * 80)

# è¼‰å…¥æ¨¡å‹
print("è¼‰å…¥æ¨¡å‹...")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=MODEL_NAME,
    max_seq_length=MAX_SEQ_LENGTH,
    dtype=None,
    load_in_4bit=True,
)
print("âœ“ æ¨¡å‹è¼‰å…¥å®Œæˆ\n")

# é…ç½® LoRA
print("é…ç½® LoRA...")
model = FastLanguageModel.get_peft_model(
    model,
    r=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha=64,
    lora_dropout=0.1,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=42,
)
print("âœ“ LoRA é…ç½®å®Œæˆ\n")

# è¼‰å…¥è³‡æ–™
print("è¼‰å…¥è¨“ç·´è³‡æ–™...")
dataset = load_dataset("json", data_files=TRAIN_FILE, split="train")
print(f"âœ“ è¨“ç·´è³‡æ–™ï¼š{len(dataset)} æ¢\n")

# é è™•ç†ï¼šç›´æ¥è½‰æˆæ–‡å­—
print("é è™•ç†è³‡æ–™...")
def convert_to_text(example):
    """ç›´æ¥è½‰æˆ ChatML æ ¼å¼çš„æ–‡å­—"""
    messages = example["messages"]
    text = ""
    for msg in messages:
        text += f"<|im_start|>{msg['role']}\n{msg['content']}<|im_end|>\n"
    return {"text": text}

dataset = dataset.map(convert_to_text, remove_columns=["messages"])
print("âœ“ é è™•ç†å®Œæˆ\n")

# é¡¯ç¤ºç¯„ä¾‹
print("ç¯„ä¾‹è³‡æ–™ï¼ˆå‰ 300 å­—å…ƒï¼‰ï¼š")
print("-" * 80)
print(dataset[0]["text"][:300])
print("...")
print("-" * 80)
print()

# è¨“ç·´
print("é–‹å§‹è¨“ç·´...")
print("=" * 80)
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=MAX_SEQ_LENGTH,
    packing=False,
    args=TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=2,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=8,
        learning_rate=2e-4,
        warmup_steps=50,
        logging_steps=10,
        save_steps=500,
        save_total_limit=2,
        bf16=True,
        optim="adamw_8bit",
        report_to="none",
        seed=42,
    ),
)

trainer.train()

# å„²å­˜
print("\nå„²å­˜æ¨¡å‹...")
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print(f"âœ“ æ¨¡å‹å·²å„²å­˜åˆ°ï¼š{OUTPUT_DIR}")

# åˆä½µæ¨¡å‹
print("åˆä½µæ¨¡å‹...")
try:
    model.save_pretrained_merged(f"{OUTPUT_DIR}_merged", tokenizer, save_method="merged_16bit")
    print(f"âœ“ åˆä½µæ¨¡å‹å·²å„²å­˜åˆ°ï¼š{OUTPUT_DIR}_merged")
except:
    print("âš  åˆä½µå¤±æ•—ï¼ˆå¯è·³éï¼‰")

print("\n" + "=" * 80)
print("âœ“ è¨“ç·´å®Œæˆï¼")
print("=" * 80)

